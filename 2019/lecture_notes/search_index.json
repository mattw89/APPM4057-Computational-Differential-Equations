[
["index.html", "Computational Differential Equations APPM4057 Preface", " Computational Differential Equations APPM4057 Dr. Matthew Woolway &amp; Krupa Prag 2019-11-12 Preface This is the set of course notes for APPM4057, authored, compiled and prepared by Professor Charis Harley and edited by Dr Matthew Woolway and Krupa Prag. The notes are intended to complement the lectures and other course material, and are by no means intended to be complete. Students should consult the various references that have been given to find additional material, and different views on the subject matter. This material is under development. Please would you report any errors or problems (no matter how big or small). Any suggestions would be gratefully appreciated. School of Computer Science and Applied Mathematics, University of the Witwatersrand, Johannesburg, South Africa This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs License. To view a copy of this license, visit, http://creativecommons.org/licenses/by-nc-nd/2.5/ Send a letter to Creative Commons, 543 Howard Street, 5th Floor, San Francisco, California, 94105, USA. In summary, this means you are free to make copies provided it is not for commercial purposes and you do not change the material. Creative Commons License "],
["course-outline.html", "Course Outline", " Course Outline Staff Members Course Coordinator: Dr Ashleigh Hutchinson Lecturers: Dr Matthew Woolway &amp; Ms Krupa Prag Room Number: UG3 &amp; UG4 (Maths Sciences Building - MSB) Email: matthew.woolway@wits.ac.za, krupa.prag@wits.ac.za Timetable for Lectures and Tutorials There will be one lecture/lectorial periods per week: Wednesday: 09:00 - 12:00 (MSL 110) For consultation please arrange an appointment via email. Communication All communications between students and lecturers will be conducted via official Wits email or through SAKAI. Announcements made during lectures OR posted to SAKAI are assumed to be read and understood by all students. Course Background, Goals and Outcomes Course Background This course assumes knowledge of the following: Course material covered in the course Numerical Methods III (APPM3017) Programming skills at the intermediate level (competent) in Python. Chapter 1 Introduction: 1. Conceptual methodology 2. Consistency 3. Choice of \\(\\Delta t\\) and \\(\\Delta x\\) At the end of the Chapter: Students must be able to structure a basic finite difference approximation of a derivative. Students must be able to calculate the accumulated error associated with the use of the Taylor series expansion. Students much be able to discuss the impact of choosing different step sizes. Chapter 2 Theoretical Considerations: 1. Convergence 2. Stability 3. The Lax Theorem At the end of the Chapter: Students must be able to distinguish between the different concepts (stability, convergence and consistency) and explain their importance within the context of the solution of a problem. Chapter 3 Stability: 1. Matrix method: eigenvalue analysis 2. Explicit Scheme 3. Fourier transform 4. Von Neumann Stability Analysis At the end of the Chapter: Students must be able to establish the stability and convergence of a basic scheme and calculate the consistency of a scheme. Chapter 4 Parabolic Partial Differential Equations: 1. Derivative boundary conditions 2. Implicit methods; Crank-Nicolson method 3. Alternating Direction Implicit Schemes 4. Peaceman-Rachford scheme At the end of the Chapter the candidate must be able to: Students must be able to distinguish between the various finite difference methods and know which are most likely to work under various scenarios. Students must be able to analyse the stability, convergence and consistency of a scheme. Students must be able to apply the methods to the solution of partial differential equations and code them in Python to produce appropriate solutions. Students must be able to engage with the code in order to assess the quality of the solutions and hence schemes employed. Students must be able to assess the quality of research results from articles (of an appropriate standard for this course) and be able to reproduce the work conducted (where the methods employed are appropriate for this course). Chapter 5 Hyperbolic Partial Differential Equations: 1. Hyperbolic conservation laws 2. Characteristics and analytical solutions 3. The Euler equations 4. Numerical methods for the solution of such equations At the end of the Chapter the candidate must be able to: Students must be able to distinguish between the various finite difference methods and know which are most likely to work under various scenarios. Students must be able to analyse the stability, convergence and consistency of a scheme. Students must be able to apply the methods to the solution of partial differential equations and code them in Python to produce appropriate solutions. Students must be able to engage with the code in order to assess the quality of the solutions and hence schemes employed. Students must be able to assess the quality of research results from articles (of an appropriate standard for this course) and be able to reproduce the work conducted (where the methods employed are appropriate for this course). Assessment There will be one test, one assignment, a set of labs and a final examination. Thus, the final course mark is made up as follows: Continuous Assessment 50% Consisting of 1 test, 1 assignment and several labs Final Examination 50% The percentage allocations which form the final mark may be slightly altered depending on the amount of coursework covered. Please note that NO STUDENT MAY MISS BOTH MAJOR ASSESSMENTS. This will result in the student not being permitted to write the exam. Tests Below are the confirmed test and preliminary assignment dates: Test Date Time Test 1 28 August 2019 09:00 Test 2 9 October 2019 09:00 Examinations The Computational Differential Equations exam will take place during the November examination period. The examination timetable will be made available online via the EGO website and self-service closer to the time. Hardware Requirements The course will be very computational in nature, however, you do not need your own personal machine. MSL already has Python installed. The labs will be running the IDEs for Python (along with Jupyter) while I will be using Jupyter for easier presentation and explanation in lectures. You will at some point need to become familiar with Jupyter as the tests will be conducted in the Maths Science Labs (MSL) utilising this platform for autograding purposes. If you do have your own machine and would prefer to work from that you are more than welcome. Since all the notes and code will be presented through Jupyter please follow the following steps: Install Anaconda from here: https://repo.continuum.io/archive/Anaconda3-2019.03-Windows-x86_64.exe Make sure when installing Anaconda to set the installation to PATH when prompted (it will be deselected by default) To launch a Jupyter notebook, open the command promt (cmd) and type jupyter notebook. This should launch the browser and jupyter. If you see any proxy issues while on campus, then you will need to set the proxy to exclude the localhost. If you are not running Windows but rather Linux, then you can get Anaconda at: https://repo.continuum.io/archive/Anaconda3-2019.03-Linux-x86_64.sh To launch jupyter on a machine in MSL, open the terminal and type /usr/local/anaconda3/bin/jupyter-notebook Note: There are often a number of different python version and environments installed. To check your particular lab machine you try: python --version ## Python 3.7.4 at the terminal, or alternatively, if you have python open: import sys print(sys.version) ## 3.7.4 (default, Aug 13 2019, 20:39:44) ## [GCC 7.3.0] Remember we are using Python3 in this course! Python Resources To get up to speed with Python, please work through the following resources: Software Carpentry: http://swcarpentry.github.io/python-novice-inflammation/ Numpy for Matlab users: https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html Jupyter Notebook for Beginners - A Tutorial: https://www.dataquest.io/blog/jupyter-notebook-tutorial/ Python Numpy tutorial: https://www.guru99.com/numpy-tutorial.html Other Information There is no set textbook for this course. The handed out material will cover all required coursework. However, you are encouraged to make use of the library for additional resources on numerical methods and analysis. The Geomaths library has ample resources for your perusal. "],
["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction The focus of the course is do consider the following: introducing finite difference grids and notation for functions defined on grids introducing a finite difference approximation of a PDE with an explanation of why this may be an appropriate approximation an introduction to the treatment of several different types of boundary conditions and forcing functions implementation of the finite difference scheme for practical problems. "],
["conceptual-methodology.html", "1.1 Conceptual Methodology", " 1.1 Conceptual Methodology Consider the following initial-boundary-value problem \\[\\begin{equation} v_t = \\nu v_{xx}, \\quad x \\in (0,1), \\quad t&gt;0 \\tag{1.1} \\end{equation}\\] \\[\\begin{equation} v(x,0) = f(x), \\quad x \\in [0,1] \\tag{1.2} \\end{equation}\\] \\[\\begin{equation} v(0,t) = a(t), \\quad v(1,t) = b(t), \\quad t\\geq0 \\tag{1.3} \\end{equation}\\] where \\(f(0) = a(0)\\) and \\(f(1) = b(0)\\). We will reduce this problem to a discrete problem which we are able to solve. We begin by discretising the spatial domain by placing a grid over the domain. For convenience, we will use a uniform grid with grid spacing \\(\\triangle{x} = 1/N\\). In a similar fashion we discretise the time domain with grid spacings \\(\\triangle{t}\\). If we wish to refer to a point on the grid we call the point \\((x_n,t_m)\\) for \\(n = 0,1,2,3,...,N\\) and \\(m = 0,1,2,3,...,M\\). See Figure 1.1. FIGURE 1.1: Schematic Discretisation for \\(N = 8\\). Blue nodes indicate values known from the initial condition. Red nodes indicate values known from boundary values. Purple nodes indicate values known from both initial and boundary conditions (and should be in agreement with one another). The space-time domain of our problem will now be approximated by the lattice points in Figure 1.1. We attempt to approximate the solutions to our problems at the points on this lattice. Notationally we define \\(u_n^m\\) as the function defined at the point \\((n \\triangle{x},\\ m \\triangle{t})\\) or the lattice point \\((n,m)\\). The function \\(u_n^m\\) will be our approximation to the solution of problem (Equations (1.1)-(1.3)) at the point \\((n \\triangle{x},\\ m \\triangle{t})\\). We now have a grid which approximates our domain. The next step is to approximate the problem (Equations (1.1)-(1.3)) on this grid. We begin by noting that since \\[\\begin{equation} v_t(x,t) = \\lim_{\\triangle{t} \\rightarrow 0} \\frac{v(x,t+\\triangle{t}) - v(x,t)}{\\triangle{t}} \\tag{1.4} \\end{equation}\\] a reasonable approximation of \\(v_t(n \\triangle{x}, m \\triangle{t})\\) can be given by \\[\\begin{equation} \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}}. \\tag{1.5} \\end{equation}\\] What we need to ask is whether this expression can be used to approximate \\(v_t(n \\triangle{x}, (m+1) \\triangle{t})\\) - and the answer is yes. Note!!! \\(u\\) is often used as an approximation to \\(v\\) - the actual solution to the partial differential equation - when we discretise in these and other notes. However the moment indices \\(n\\) and \\(m\\) are introduced \\(v_n^m\\) is also seen as an approximation. In a similar fashion we approximate \\(v_{xx}\\) at \\((n \\triangle{x}, m\\triangle{t})\\) by \\[\\begin{equation} \\frac{u_{n+1}^m - 2 u_n^m + u_{n-1}^m}{\\triangle{x}^2}. \\tag{1.6} \\end{equation}\\] To verify that this is a reasonable approximation, consider that \\[\\begin{equation} \\frac{(v_x)_{n+\\frac{1}{2}}^m - (v_x)_{n-\\frac{1}{2}}^m}{\\triangle{x}} \\tag{1.7} \\end{equation}\\] approximates \\((v_{xx})_n^m\\) and that \\((v_x)_{n+\\frac{1}{2}}^m\\) and \\((v_x)_{n-\\frac{1}{2}}^m\\) can be approximated by \\[\\begin{equation} \\dfrac{u_{n+1}^m - u_n^m}{\\triangle{x}} \\quad {\\rm and} \\quad \\dfrac{u_{n}^m - u_{n-1}^m}{\\triangle{x}} \\tag{1.8} \\end{equation}\\] respectively. Then \\[(v_{xx})_n^m \\approx \\frac{(v_x)_{n+\\frac{1}{2}}^m - (v_x)_{n-\\frac{1}{2}}^m}{\\triangle{x}}\\] \\[\\begin{equation} \\dfrac{\\frac{u_{n+1}^m - u_n^m}{\\triangle{x}} - \\dfrac{u_{n}^m - u_{n-1}^m}{\\triangle{x}}}{\\triangle{x}}. \\tag{1.9} \\end{equation}\\] As a consequence, the expressions given in Equation (1.5) and (1.6) can be used to approximate the partial differential equation given by Equation (1.1) at the point \\((n \\triangle{x},m\\triangle{t})\\) as follows \\[\\begin{equation} \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}} = \\nu \\frac{u_{n+1}^m - 2 u_n^m + u_{n-1}^m}{\\triangle{x}^2} \\tag{1.10} \\end{equation}\\] and thus \\[\\begin{equation} u_n^{m+1} = u_n^m + \\nu \\frac{\\triangle{t}}{\\triangle{x}^2} \\left(u_{n+1}^m - 2 u_n^m + u_{n-1}^m \\right). \\tag{1.11} \\end{equation}\\] Finally, we can also see that the boundary conditions are approximated as follows \\[\\begin{equation} u_n^0 = f(n \\triangle{x}), \\quad n = 0,1,.... \\tag{1.12} \\end{equation}\\] \\[\\begin{equation} u_0^{m+1} = a((m+1)\\triangle{t}), \\quad m = 0,1,.... \\tag{1.13} \\end{equation}\\] and \\[\\begin{equation} u_N^{m+1} = b((m+1)\\triangle{t}), \\quad m = 0,1,.... \\tag{1.14} \\end{equation}\\] The approach taken in this course will be to obtain an approximation of the solution to the problem (1.1)-(1.3) by solving the discrete problem (1.11)-(1.14). The values of \\(\\triangle{t}\\) and \\(\\triangle{x}\\) must still be chosen, and this choice will determine much about the accuracy and behaviour of the solution. It must be noted here that from (1.12) and using (1.11) we are able to obtain \\(u_n^1\\) for \\(n = 0,1,2,...,N-1\\). Equations (1.13) and (1.14) can be used to determine \\(u_0^1\\) and \\(u_N^1\\). This process is iterative. It is important to notice that it is not possible to determine \\(u_0^1\\) and \\(u_N^1\\) from equation (1.11) since one of the subscripts \\(N+1\\) and \\(-1\\) would reach out of bounds for either of the calculations. This indicates the necessity of some form of boundary treatments. We have now developed a numerical scheme to approximate the solution of an initial-boundary-value problem (1.1)-(1.3). This scheme is called an explicit scheme since we are able to solve for the variable at the \\((m+1)st\\) time level explicitly. An important question to ask at this point is whether this is a good scheme and whether the solution is physically useful. We will answer these questions to a degree as the course develops. 1.1.1 Tutorial Write a Python fuction to approximately solve the problem given by (1.1)-(1.3). Use \\(f(x) = \\sin 2 \\pi x\\), \\(a=b=0\\), \\(M=10\\) and \\(\\nu = 1/6\\). Find solutions at \\(t = 0.06\\), \\(t = 0.9\\) and \\(t = 50\\). Experiment with various values of \\(\\triangle{t}\\) and \\(\\triangle{x}\\). Solution: The surface plot of the solution should look like this: Furthermore, we can see that solving the temperature at each time step that the heat is decreasing over time: "],
["sec-consis.html", "1.2 Consistency", " 1.2 Consistency We will now proceed to investigate how good a scheme (1.11)-(1.14) is for approximating the solution to the problem (1.1)-(1.3). A starting point is to see how well the difference equation (1.11) approximates the partial differential equation (1.1). We will use the Taylor series to investigate this. To obtain the finite difference equation used to approximate (1.1) we used the approximation \\[\\begin{equation} v_t(n \\triangle{x}, m\\triangle{t}) \\approx \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}}. \\tag{1.15} \\end{equation}\\] Using the Taylor series we find that \\[v_n^{m+1} = v(n \\triangle{x},(m+1)\\triangle{t})\\] \\[\\begin{equation} = v(n \\triangle{x},m\\triangle{t}) + \\frac{\\partial v}{\\partial t}(n \\triangle{x},m\\triangle{t})\\frac{\\triangle{t}}{1!} + \\frac{\\partial ^2 v}{\\partial t^2}(n \\triangle{x},m\\triangle{t})\\frac{\\triangle{t}^2}{2!} + \\cdots. \\tag{1.16} \\end{equation}\\] And as such \\[\\begin{equation} \\frac{v_n^{m+1} - v_n^m}{\\triangle{t}} = \\frac{\\partial v}{\\partial t}(n \\triangle{x},m\\triangle{t}) + \\frac{\\triangle{t}}{2} \\left(\\frac{\\partial ^2 v}{\\partial t^2}\\right)_n^m + \\cdots. \\tag{1.17} \\end{equation}\\] We generally write this as \\[\\begin{equation} \\frac{v_n^{m+1} - v_n^m}{\\triangle{t}} = \\frac{\\partial v}{\\partial t}(n \\triangle{x},m\\triangle{t}) + \\mathcal{O}(\\triangle{t}) \\tag{1.18} \\end{equation}\\] where the expression assumes that the higher order derivatives of \\(v\\) at \\((n \\triangle{x}, m\\triangle{t})\\) are bounded. The definition of \\(\\mathcal{O}\\) is that \\(f(s) = \\mathcal{O}(\\phi(s))\\) for \\(s\\in S\\) if there exists a constant \\(A\\) such that \\(|f(s)|\\leq A|\\phi(s)|\\) for all \\(s\\in S\\). We say that \\(f(x)\\) is the “big \\(\\mathcal{O}\\)” of \\(\\phi(x)\\) or that \\(f(s)\\) is of order \\(\\phi(s)\\). The above Taylor series expansion of \\(v(n \\triangle{x},m\\triangle{t})\\) shows what we are throwing away when we replace \\(v_t\\) in the partial differential equation by \\(\\frac{u_n^{m+1} - u_n^m}{\\triangle{t}}\\). It must be noted that the constant associated with \\(\\mathcal{O}\\) can be very large. But generally, if \\(\\triangle{t}\\) is sufficiently small, we are neglecting terms that are of order \\(\\triangle{t}\\), so \\(\\frac{u_n^{m+1} - u_n^m}{\\triangle{t}}\\) is a good approximation of \\(v_t\\). The same approach can be used to show that \\[\\begin{equation} \\frac{v_{n+1}^m - v_n^m}{\\triangle{x}} = \\frac{\\partial v}{\\partial x}(n \\triangle{x},m\\triangle{t}) + \\mathcal{O}(\\triangle{x}) \\tag{1.19} \\end{equation}\\] \\[\\begin{equation} \\frac{v_{n}^m - v_{n-1}^m}{\\triangle{x}} = \\frac{\\partial v}{\\partial x}(n \\triangle{x},m\\triangle{t}) + \\mathcal{O}(\\triangle{x}) \\tag{1.20} \\end{equation}\\] and \\[\\begin{equation} \\frac{v_{n+1}^m - v_{n-1}^m}{2\\triangle{x}} = \\frac{\\partial v}{\\partial x}(n \\triangle{x},m\\triangle{t}) + \\mathcal{O}(\\triangle{x}^2) \\tag{1.21} \\end{equation}\\] where (1.20) is termed the backward difference approximation and (1.21) is termed the central difference approximation to the respective derivatives. Here we can see that the centred difference given by (1.21) approximates the first derivative with respect to \\(x\\) more accurately than either of the one sided differences: \\(\\mathcal{O}(\\triangle{x}^2)\\) vs. \\(\\mathcal{O}(\\triangle{x})\\). Again using the Taylor series expansion we can show that \\[\\begin{equation} \\frac{v_{n+1}^m - 2 v_n^m + v_{n-1}^m}{\\triangle{x}^2} = \\frac{\\partial^2 v }{\\partial x^2}(n \\triangle{x},m\\triangle{t}) + \\mathcal{O}(\\triangle{x}^2). \\tag{1.22} \\end{equation}\\] The order of approximation found in (1.22) makes sense in the derivation of \\(\\frac{u_{n+1}^m - 2 u_n^m + u_{n-1}^m}{\\triangle{x}^2}\\) as an approximation to \\(\\frac{\\partial^2 v }{\\partial x^2}\\); all differences used were centered differences. Returning to the partial differential equation (1.1) we find that \\[v_t(n \\triangle{x},m\\triangle{t}) - \\nu v_{xx}(n \\triangle{x},m\\triangle{t})\\] \\[\\begin{equation} = \\frac{v_n^{m+1} - v_n^m}{\\triangle{t}} - \\frac{\\nu}{\\triangle{x}^2} \\left(v_{n+1}^m - 2 v_n^m + v_{n-1}^m \\right) + \\mathcal{O}(\\triangle{t}) + \\mathcal{O}(\\triangle{x}^2).\\tag{1.23} \\end{equation}\\] Thus we find that difference equation (1.10) approximates (1.1) to the first order in \\(\\triangle{t}\\) and the second order in \\(\\triangle{x}\\). We can also view equation (1.23) as follows: if \\(v = v(x,t)\\) is a solution to the partial differential equation (so that the left hand side of equation ((1.23) is zero), then \\(v\\) satisfies the difference equation to the first order in \\(\\triangle{t}\\) and the second order in \\(\\triangle{x}\\). Equation (1.23) shows how well the difference equation approximates the partial differential equation. However, this does not tell us how well the solution to the difference equation will approximate the solution to the partial differential equation. Definition 1.1 Given a partial differential equation \\(Pu = f\\) and a finite difference scheme, \\(P_{\\Delta t, \\Delta x}v = f\\), we say that the finite difference scheme is consistent with the partial differential equation if for any smooth function \\(u(x, t)\\): \\[ Pu - Pu_{\\Delta t, \\Delta x }v \\rightarrow 0\\quad \\text{as} \\Delta t, \\Delta x \\rightarrow 0. \\] Example 1.1 Given the one-way wave equation produced by the operation \\(P = \\frac{\\partial}{\\partial t} + \\alpha\\frac{\\partial}{\\partial x}\\): \\[ Pu = u_t + \\alpha u_x, \\quad \\alpha &gt; 0. \\] We can evaluate the consistency of the FTFS scheme with the difference operator \\(P_{\\Delta t, \\Delta x}\\): \\[ P_{\\Delta t, \\Delta x}u = \\dfrac{u_{i}^{n+1} - u_{i}^{n}}{\\Delta t} + \\alpha\\dfrac{u_{i+1}^{n} - u_{i}^{n}}{\\Delta x}, \\] where: \\[ u_i^n = u(i\\Delta x, n\\Delta t). \\] Now we take the Taylor series expansions of \\(u\\) in terms of \\(t\\) and \\(x\\) about \\((t_n, x_i)\\). Thus: \\[ u_{i}^{n+1} = u_{i}^{n} + \\Delta t u_t + \\dfrac{1}{2}\\Delta t^2 u_{tt} + \\mathcal{O}(\\Delta t^3), \\] \\[ u_{i+1}^{n} = u_{i}^{n} + \\Delta x u_x + \\dfrac{1}{2}\\Delta x^2 u_{xx} + \\mathcal{O}(\\Delta t^3). \\] Therefore: \\[ P_{\\Delta t, \\Delta x}u = u_t + \\alpha u_x + \\dfrac{1}{2}\\Delta t u_{tt} + \\alpha\\dfrac{1}{2}\\Delta x u_{xx} + \\mathcal{O}(\\Delta t^2) + \\mathcal{O}(\\Delta x^2). \\] Hence: \\[ Pu - P_{\\Delta t, \\Delta x}u = \\dfrac{1}{2}\\Delta t u_{tt} + \\dfrac{1}{2}\\Delta x u_{xx} + \\mathcal{O}(\\Delta t^2) + \\mathcal{O}(\\Delta x^2) \\rightarrow 0\\quad \\text{as} (\\Delta x, \\Delta t) \\rightarrow 0. \\] Hence the scheme is consistent. \\(\\square\\) 1.2.1 Tutorial Write a Python function to approximately solve the problem given by (1.1)-(1.3) using the following leapfrog scheme \\[u_n^{m+1} = u_n^{m-1} + \\frac{2 \\nu \\triangle{t}}{\\triangle{x}^2} \\left(u_{n+1}^m - 2 u_n^m + u_{n-1}^m \\right)\\] where \\(f(x) = \\sin 2 \\pi x\\), \\(a=b=0\\), \\(M=10\\) and \\(\\nu = 1/6\\). Find solutions at \\(t = 0.02\\). Experiment with various values of \\(\\triangle{t}\\) and \\(\\triangle{x}\\). [Hint: Your will need to use the condition \\(u_t |_{t=0} = 0\\).] How would you derive the leapfrog scheme? Evaluate the consistency of the FTCS scheme when applied to the heat equation. "],
["choice-of-trianglet-and-trianglex.html", "1.3 Choice of \\(\\triangle{t}\\) and \\(\\triangle{x}\\)", " 1.3 Choice of \\(\\triangle{t}\\) and \\(\\triangle{x}\\) As we have seen, different difference operators can be used to give higher order approximations of the partial differential equation. The choice of \\(\\triangle{t}\\) and \\(\\triangle{x}\\) becomes important in this regard, since a scheme can be improved depending on the particular choice of these step-sizes. In fact, by a particular choice of \\(\\triangle{t}\\) and \\(\\triangle{x}\\), the order of the scheme given in equation (1.11) can go from first order in time and second order in space to second order in time and fourth order in space. It is important to be aware of the impact of our choice for \\(\\triangle{t}\\) and \\(\\triangle{x}\\) since it can explain why results obtained from a scheme can at times be more accurate than we would expect them to be. We will now show how an improvement on the order of the accuracy can be obtained. We begin by expanding the terms in equation (1.23) to see that \\[\\left(v_t - \\nu v_{xx} \\right) _n^m - \\left[\\frac{v_{n}^{m+1}- v_n^m}{\\triangle{t}} - \\nu \\frac{v_{n+1}^m - 2 v_n^m + v_{n-1}^m}{\\triangle{x}^2} \\right]\\] \\[= -\\frac{\\triangle{t}}{2} \\left(v_{tt}\\right)_n^m - \\frac{\\triangle{t}^2}{3!} \\left(v_{ttt}\\right)_n^m - \\cdots\\] \\[\\begin{equation} +\\nu \\left[2 \\frac{\\triangle{x}^2}{4!} \\left(v_{xxxx}\\right)_n^m + 2 \\frac{\\triangle{x}^4}{6!} \\left(v_{xxxxxx}\\right)_n^m + \\cdots \\right]. \\tag{1.24} \\end{equation}\\] Now, since \\(v_t = \\nu v_{xx}\\) we find that \\[\\begin{equation} v_{tt} = \\nu v_{xxt} = \\nu (v_t)_{xx} = \\nu (\\nu v_{xx})_{xx} = \\nu^2 v_{xxxx}. \\tag{1.25} \\end{equation}\\] Using this expression on the right hand side of (1.24) gives \\[\\left(v_t - v_{xx} \\right) _n^m - \\left[\\frac{v_{n}^{m+1}- v_n^m}{\\triangle{t}} - \\nu \\frac{v_{n+1}^m - 2 v_n^m + v_{n-1}^m}{\\triangle{x}^2} \\right]\\] \\[=-\\frac{\\triangle{t}}{2} \\nu^2 (v_{xxxx})_n^m - \\frac{\\triangle{t}^2}{3!} (v_{ttt})_n^m - \\cdots\\] \\[+ 2 \\nu \\frac{\\triangle{x}^2}{4!} \\left(v_{xxxx}\\right)_n^m + \\nu \\left[2\\frac{\\triangle{x}^4}{6!} \\left(v_{xxxxxx}\\right)_n^m + \\cdots \\right]\\] \\[\\begin{equation} = \\nu \\left(-\\nu \\frac{\\triangle{t}}{2} + 2 \\frac{\\triangle{x}^2}{4!} \\right) (v_{xxxx})_n^m + \\mathcal{O}(\\triangle{t}^2) + \\mathcal{O}(\\triangle{x}^4). \\tag{1.26} \\end{equation}\\] Hence if we choose \\(\\triangle{t} = \\frac{\\triangle{x}^2}{6 \\nu}\\) we will have a scheme that is second order in time and fourth order in space. 1.3.1 Tutorial Use the above choice for \\(\\triangle{t}\\) in your solution to the problem (1.1) - (1.3). Compare and contrast the error in these results to the error in your results obtained before. "],
["theorectical-considerations.html", "Chapter 2 Theorectical Considerations", " Chapter 2 Theorectical Considerations This chapter is focussed towards establishing some basic definitions of what it means for the solution of the finite difference equations to converge to the solution of the partial differential equation. Understanding what type of convergence the scheme has and how this affects its accuracy is important. The most common approach to convergence is through an investigation of the notions of consistency and stability and the Lax Theorem. "],
["convergence.html", "2.1 Convergence", " 2.1 Convergence We need the solution of the difference equations to be able to approximate the solution of the partial differential equation to any desired accuracy. Thus we want some sort of convergence of the solution of the finite difference equation to the solution of the partial differential equation. Take note of the following definitions: Definition 2.1 A difference scheme \\(L_n^m u_n^m = G_n^m\\) approximating the partial differential equation \\(\\mathcal{L}v = F\\) is a pointwise convergent scheme if for any \\(x\\) and \\(t\\), as \\((n\\triangle{x},(m+1)\\triangle{t})\\) converges to \\((x,t)\\), \\(u_n^m\\) converges to \\(v(x,t)\\) as \\(\\triangle{x}\\) and \\(\\triangle{t}\\) converge to \\(0\\). To demonstrate this definition we prove convergence for the explicit scheme given by \\[\\begin{equation} u_n^{m+1} = u_n^m +\\nu \\frac{\\triangle{t}}{\\triangle{x}^2} \\left( u_{n+1}^m - 2 u_n^m + u_{m-1}^m \\right). \\tag{2.1} \\end{equation}\\] To show that the solution of the difference scheme converges pointwise to the solution of the initial-value problem \\[v_t = \\nu v_{xx} \\qquad x \\in \\mathbb{R}, \\qquad t&gt;0\\] \\[\\begin{equation} v(x,0) = f(x) \\qquad x \\in \\mathbb{R} \\end{equation}\\] we consider (2.1) as \\[\\begin{equation} u_n^{m+1} = (1-2 \\beta) u_n^m + \\beta(u_{n+1}^m + u_{n-1}^m) \\tag{2.2} \\end{equation}\\] \\[\\begin{equation} u_n^0 = f(n \\triangle{x}) \\tag{2.3} \\end{equation}\\] where \\(\\beta = \\nu \\frac{\\triangle{t}}{\\triangle{x}^2}\\). Since we are considering an initial-value problem on all \\(\\mathbb{R}\\), the \\(n\\) index on \\(u_n^m\\) will range over all integers \\(-\\infty&lt;n&lt;\\infty\\). We denote the exact solution to the initial-value problem (2.2)-(2.3) by \\(v=v(x,t)\\) and set \\[\\begin{equation} z_n^m = u_n^m - v(n\\triangle{x}, m\\triangle{t}). \\tag{2.4} \\end{equation}\\] If we insert \\(v\\) into equation (1.23) (so that it makes the left hand side of equation (1.23) zero) and multiply through by \\(\\triangle{t}\\), we see that \\(v_n^m = v(n\\triangle{x},m\\triangle{t})\\) satisfies \\[\\begin{equation} v_n^{m+1} = (1-2\\beta)v_n^m + \\beta(v_{n+1}^m + v_{n-1}^m) + \\mathcal{O}(\\triangle{t}^2) + \\mathcal{O}(\\triangle{t}\\triangle{x}^2). \\tag{2.5} \\end{equation}\\] Then by subtracting equation (2.5) from equation (2.2) we see that \\(z_n^m\\) satisfies \\[\\begin{equation} z_n^{m+1} = (1-2\\beta)z_n^m + \\beta(z_{n+1}^m + z_{n-1}^m) + \\mathcal{O}(\\triangle{t}^2) + \\mathcal{O}(\\triangle{t}\\triangle{x}^2). \\tag{2.6} \\end{equation}\\] If \\(0&lt;\\beta \\leq 1/2\\) the coefficients on the right hand side of equation (2.6) are non-negative and \\[|z_n^{m+1}|\\leq (1-2\\beta)|z_n^m| + \\beta|z_{n+1}^m| + \\beta |z_{n-1}^m| + A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2)\\] \\[\\begin{equation} \\leq Z^m + A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2) \\tag{2.7} \\end{equation}\\] where \\(A\\) is the constant associated with the ``big \\(\\mathcal{O}\\)&quot; terms and depends on the assumed bounds of the higher order derivatives of \\(v\\) and \\(Z^m = \\sup_n \\{|z_n^m|\\}\\). Then taking the sup over \\(n\\) on the left side of the above equation gives \\[\\begin{equation} Z^{m+1}\\leq Z^m+A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2). \\tag{2.8} \\end{equation}\\] Take note that taking the supremum over the right hand side included the terms containing \\(\\triangle{t}\\) and \\(\\triangle{x}\\). In this case we are assuming that the constant \\(A\\) is a bound of the second derivative with respect to time and the fourth derivative with respect to space on the entire real line. Thus we have assumed that these appropriate derivatives of the solution are uniformly bounded. Applying (2.8) repeatedly yields \\[Z^{m+1}\\leq Z^m+A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2)\\] \\[ \\leq Z^{m-1} + 2A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2)\\] \\[\\vdots\\] \\[\\begin{equation} \\leq Z^{0} + (m+1)A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2). \\tag{2.9} \\end{equation}\\] Since \\(Z^0 = 0\\), \\(|u_n^{m+1} - v(n\\triangle{x},(m+1)\\triangle{t})|\\leq Z^{m+1}\\) and \\((m+1)\\triangle{t}\\rightarrow t\\) we have that \\[|u_n^{m+1} - v(n\\triangle{x},(m+1)\\triangle{t})| \\leq (m+1)A(\\triangle{t}^2 + \\triangle{t}\\triangle{x}^2)\\] \\[\\begin{equation} \\rightarrow 0 \\tag{2.10} \\end{equation}\\] as \\(\\triangle{t}, \\triangle{x} \\rightarrow 0\\). Thus we see that for any \\(x\\) and \\(t\\) as \\(\\triangle{t}\\) and \\(\\triangle{x}\\) approach \\(0\\) in such a way that \\((n\\triangle{x},(m+1)\\triangle{t}) \\rightarrow (x,t)\\), \\(u_n^m\\) approaches \\(v(x,t)\\). Take note that the \\(m+1\\) expression in (2.10) could cause problems for our convergence since it is not good to have terms that go to infinity in an expression that you want to go to zero. Thus it was important as a last step that \\((m+1)\\triangle{t} \\rightarrow t\\). Furthermore, notice that we assumed that \\(0&lt;\\beta \\leq 1/2\\) and this assumption was a very important one in this proof - we are not saying however, that convergence could not have been proven without this assumption. This condition on \\(\\beta\\) places a condition on \\(\\triangle{t}\\) which means we notice that \\(\\triangle{t} \\leq \\triangle{x}^2/2\\nu\\). "],
["consistency.html", "2.2 Consistency", " 2.2 Consistency As before denote \\(\\mathcal{L}v = F\\) as the partial differential equation under consideration and \\(L_n^m u_n^m = G_n^m\\) as the difference approximation. Take note of the following Definitions. Definition 2.2 The finite difference scheme \\(L_n^m u_n^m = G_n^m\\) is pointwise consistent with the partial differential equation \\(\\mathcal{L}v = F\\) at point \\((x,t)\\) if for any smooth function \\(\\phi = \\phi(x,t)\\), \\[(\\mathcal{L}\\phi-F)|_n^m - [L_n^m \\phi(n \\triangle{x}, m \\triangle{t})-G_n^m ] \\rightarrow 0\\] as \\(\\triangle{x}, \\triangle{t}\\rightarrow 0\\) and \\((n \\triangle{x}, (m+1) \\triangle{t})\\rightarrow (x,t)\\). A stronger definition of consistency can be given as follows. Definition 2.3 The finite difference scheme \\({\\bf u}^{m+1} = Q {\\bf u}^m + \\triangle{t} {\\bf G}^m\\) is consistent with the partial differential equation in a norm \\(||\\cdot||\\) if the solution of the partial differential equation, \\(v\\), satisfies \\[{\\bf v}^{m+1} = Q {\\bf v}^m + \\triangle{t} {\\bf G}^m + \\triangle{t} {\\bf \\tau}^m\\] and \\[||{\\bf \\tau}^m||\\rightarrow0\\] as \\(\\triangle{x}, \\triangle{t} \\rightarrow 0\\), where \\({\\bf v}^m\\) denotes the vector whose \\(n\\)th component is \\(v(n\\triangle{x}, m\\triangle{t})\\).} A slight variation of the definition of consistency is given below where the order in which \\(\\tau^m\\) goes to zero (which is the same order to which the finite difference scheme approximates the partial differential equation) is included. Definition 2.4 The finite difference scheme \\({\\bf u}^{m+1} = Q {\\bf u}^m + \\triangle{t} {\\bf G}^m\\) is said to be accurate of order \\((p.q)\\) to the given partial differential equation if \\[\\lVert{\\bf \\tau}^m\\rVert = \\mathcal{O}(\\triangle{x}^p) + \\mathcal{O}(\\triangle{t}^q).\\] We refer to \\({\\bf \\tau}^m\\) or \\(||{\\bf \\tau}^m||\\) as the truncation error. Let us now consider the following example \\[\\begin{equation} \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}} = \\nu \\frac{u_{n+1}^m - 2 u_n^m + u_{n-1}^m}{\\triangle{x}^2}\\tag{2.11} \\end{equation}\\] which is an explicit difference scheme and discuss its consistency with the partial differential equation \\[\\begin{equation} v_t = \\nu v_{xx}, \\qquad -\\infty&lt;x&lt;\\infty, \\qquad t&gt;0. \\tag{2.12} \\end{equation}\\] If we denote the solution of equation (2.12) by \\(v\\) and insert \\(v\\) into expression (2.11) we see that the left hand side of expression (1.23) is zero and we are left with \\[\\begin{equation} \\frac{v_n^{m+1} - v_n^m}{\\triangle{t}} - \\frac{\\nu}{\\triangle{x}^2} (v_{n+1}^m - 2 v_n^m + v_{n-1}^m) = \\mathcal{O}(\\triangle{t}) + \\mathcal{O}(\\triangle{x}^2) \\tag{2.13} \\end{equation}\\] where we now let \\(\\beta = \\nu \\triangle{t}/\\triangle{x}^2\\). To apply Definitions 1-3 we need to be sure of what is contained in the \\(\\mathcal{O}(\\triangle{t}) + \\mathcal{O}(\\triangle{x}^2)\\) term. In Section 1.2 we vaguely replaced the tail ends of the Taylor series expansions by the “big \\(\\mathcal{O}\\)” notation and quit. If we now rather replace the series used in Section 1.2 by the Taylor series with a remainder it is then clear that the ``big \\(\\mathcal{O}\\)&quot; terms contain a second derivative with respect to \\(t\\) evaluated at \\(x = n \\triangle{x}\\) and some \\(t\\) in a neighbourhood of \\(m\\triangle{t}\\) and a fourth derivative with respect to \\(x\\) evaluated at \\(t=m\\triangle{t}\\) and for some \\(x\\) in a neighbourhood of \\(n\\triangle{x}\\). If we then assume that the second derivative of \\(v\\) with respect to \\(t\\) and the fourth derivative of \\(v\\) with respect to \\(x\\) exist and are bounded in some neighbourhood of the point \\((x,t)\\), then expression (2.13) will imply that the difference scheme (2.11) is pointwise consistent with the partial differential equation (2.12). To show that the difference scheme (2.11) is consistent, or accurate of order \\((2,1)\\), we must first write the scheme in the form of \\[\\begin{equation} {\\bf u}^{m+1} = Q {\\bf u}^m + \\triangle{t} {\\bf G}^m. \\tag{2.14} \\end{equation}\\] To do so we multiply through by \\(\\triangle{t}\\), solve for \\(u_n^{m+1}\\) to obtain \\[\\begin{equation} u_n^{m+1} = u_n^m + \\nu \\frac{\\triangle{t}}{\\triangle{x}^2}(u_{n+1}^m - 2 u_n^m + u_{n-1}^m). \\tag{2.15} \\end{equation}\\] The above equation gives each component of an equation in the form of (2.14). Hence, to apply Definition 2 or 3 we let \\(v\\) be the solution to the partial differential equation (2.12) and write \\[\\triangle{t} \\tau_n^m = v_n^{m+1} - \\{v_n^m + \\beta[v_{n+1}^m - 2v_n^m + v_{n-1}^m]\\}\\] \\[ = v_n^m + (v_t)_n^m\\triangle{t} + v_{tt}(n\\triangle{x},t_1)\\frac{\\triangle{t}^2}{2}\\] \\[ - (v_n^m + \\beta[v_n^m + (v_x)_n^m \\triangle{x} + (v_{xx})_n^m \\frac{\\triangle{x}^2}{2} + (v_{xxx})_n^m\\frac{\\triangle{x}^3}{6}\\] \\[+ v_{xxxx}(x_1,m\\triangle{t})\\frac{\\triangle{x}^4}{24} - 2v_n^m + v_n^m - (v_x)_n^m \\triangle{x}\\] \\[ +(v_{xx})_n^m \\frac{\\triangle{x}^2}{2} - (v_{xxx})_n^m \\frac{\\triangle{x}^3}{6} + v_{xxxx}(x_2,m\\triangle{t}) \\frac{\\triangle{x}^4}{24}])\\] \\[ =(v_t)_n^m\\triangle{t} - \\beta\\triangle{x}^2 (v_{xx})_n^m + v_{tt}(n\\triangle{x},t_1)\\frac{\\triangle{t}^2}{2}\\] \\[\\begin{equation} - \\beta v_{xxxx}(x_1,m\\triangle{t}) \\frac{\\triangle{x}^4}{24} - \\beta v_{xxxx}(x_2,m\\triangle{t}) \\frac{\\triangle{x}^4}{24} \\tag{2.16} \\end{equation}\\] \\[ =(v_t-\\nu v_{xx})_n^m\\triangle{t} + v_{tt}(n\\triangle{x},t_1)\\frac{\\triangle{t}^2}{2}\\] \\[\\begin{equation} -\\nu v_{xxxx}(x_1,m\\triangle{t}) \\frac{\\triangle{x}^2}{24}\\triangle{t} - \\nu v_{xxxx}(x_2,m\\triangle{t}) \\frac{\\triangle{x}^2}{24} \\triangle{t} \\tag{2.17} \\end{equation}\\] where \\(t_1\\), \\(x_1\\) and \\(x_2\\) are the appropriate points given to us from the Taylor series remainder term and (2.17) follows from (2.16) because \\(\\beta = \\nu \\triangle{t}/\\triangle{x}^2\\). Since \\(v_t - \\nu v_{xx} = 0\\) we see that \\[\\begin{equation} \\tau_n^m = v_{tt}(n\\triangle{x},t_1) \\frac{\\triangle{t}}{2} - \\nu (v_{xxxx}(x_1,m\\triangle{t}) + v_{xxxx}(x_2,m\\triangle{t}))\\frac{\\triangle{x}^2}{24}. \\tag{2.18} \\end{equation}\\] To apply the above result we must now decide which norm we will use. If we assume that \\(v_{tt}\\) and \\(v_{xxxx}\\) are uniformly bounded on \\(\\mathbb{R} \\times [0,t_0]\\) (for some \\(t_0&gt;0\\)) we can then use these bounds along with the sup-norm to get that the scheme is accurate of order (2,1) with respect to the sup-norm. If we assume that \\(v_{tt}\\) and \\(v_{xxxx}\\) satisfy \\[\\begin{equation} \\sum _{n=-\\infty}^\\infty [(v_{tt})_n^m]^2 \\triangle{t} &lt; A &lt; \\infty \\tag{2.19} \\end{equation}\\] and \\[\\begin{equation} \\sum _{n=-\\infty}^\\infty [(v_{xxxx})_n^m]^2 \\triangle{x} &lt; B &lt; \\infty \\tag{2.20} \\end{equation}\\] for any \\(\\triangle{x}\\) and \\(\\triangle{t}\\) then we see that the difference scheme is accurate order (2,1) with respect to the \\({\\it l}_{2,\\triangle{x}}\\) norm. "],
["stability.html", "2.3 Stability", " 2.3 Stability In order to prove convergence we need to show that the scheme is consistent and stable (this shall be discussed at a later stage in this chapter). One interpretation of the stability of a difference scheme is that for a stable difference scheme small errors in the initial conditions cause small errors in the solution. The definition allows them to grow, but in a bounded fashion, no faster that exponential. We will define stability for a two level difference scheme of the form \\[\\begin{equation} {\\bf u}^{m+1} = Q {\\bf u}^m, m\\geq 0 \\tag{2.21} \\end{equation}\\] which is generally a difference scheme for solving a given initial-value problem on \\(\\mathbb{R}\\). \\\\ Definition 2.5 The difference scheme (2.21) is said to be stable with respect to the norm \\(||\\cdot||\\) if there exist positive constants \\(\\triangle{x}_0\\) and \\(\\triangle{t}_0\\), and non-negative constants \\(K\\) and \\(\\beta\\) so that \\[\\lVert{\\bf u}^{m+1}\\rVert\\leq K e^{\\beta t} \\lVert{\\bf u}^0\\rVert\\] for \\(0\\leq t=(m+1)\\triangle{t}\\), \\(0\\leq \\triangle{x} \\leq \\triangle{x}_0\\) and \\(0\\leq \\triangle{t} \\leq \\triangle{t}_0\\). Proposition 2.1 The difference scheme (2.21) is stable with respect to the norm \\(\\lVert\\cdot\\rVert\\) if and only if there exists positive constants \\(\\triangle{x}_0\\) and \\(\\triangle{t}_0\\) and non-negative constants \\(K\\) and \\(\\beta\\) so that \\[\\lVert Q^{m+1}\\rVert\\leq K e^{\\beta t}\\] for \\(0\\leq t=(m+1)\\triangle{t}\\), \\(0\\leq \\triangle{x} \\leq \\triangle{x}_0\\) and \\(0\\leq \\triangle{t} \\leq \\triangle{t}_0\\). "],
["the-lax-theorem.html", "2.4 The Lax Theorem", " 2.4 The Lax Theorem This theorem will show the connection between the notions of convergence, consistency and stability. Theorem 2.1 (Lax Equivalence Theorem) A consistent, two level difference scheme for a well-posed linear initial-value problem is convergent if and only if it is stable. Theorem 2.2 (Lax Theorem) If a two level difference scheme \\[{\\bf u}^{m+1} = Q {\\bf u}^m + \\triangle{t} {\\bf G}^m\\] is accurate of order \\((p,q)\\) in the norm \\(||\\cdot||\\) to a well-posed linear initial-value problem and is stable with respect to the norm \\(||\\cdot||\\), then it is convergent of order \\((p,q)\\) with respect to the norm \\(||\\cdot||\\). Theorem 2.3 (Lax Theorem) If a two level difference scheme is accurate of order \\((p,q)\\) in the sequence of norms \\({||\\cdot||_j}\\) to a well-posed linear initial-boundary-value problem and is stable with respect to the sequence of norms \\({||\\cdot||_j}\\) , then it is convergent of order \\((p,q)\\) with respect to the sequence of norms \\({||\\cdot||_j}\\). FIGURE 2.1: Relations between consistency, stability and convergence. The interrelations between convergence, consistency and stability are summarized in Figure 2.1 which expresses, in short, that the consistency condition defines a relation between the differential equation and its discrete formulation; that the stability condition establishes a relation between the computed solution and the exact solution of the discretized equations; while the convergence condition connects the computed solution to the exact solution of the differential equation. Remark: It is important that the norms used in consistency and stability be the same and that the convergence is given with respect to that norm. Also, considering only pointwise consistency is not sufficient. It can be proved that if \\(\\beta = \\frac{\\triangle{t}}{\\triangle{x}^2} \\leq \\frac{1}{2}\\) then the explicit scheme \\(u_n^{m+1} = (1-2\\beta)u_n^m + \\beta(u_{n+1}^m + u_{n-1}^m)\\) is both accurate of order \\((2,1)\\) and stable with respect to the sup-norm. By the Lax Theorem above: if \\(\\beta\\leq1/2\\) then the explicit scheme is convergent of order \\((2,1)\\) with respect to the sup-norm. "],
["stability-1.html", "Chapter 3 Stability", " Chapter 3 Stability Due to rounding errors in calculations we do not produce the exact solution \\(v\\) for the partial differential equations we consider, but rather another approximate value \\(u\\). It is important to make sure that \\(v-u\\) is bounded, hence that the errors involved remain bounded. \\[\\begin{equation} v_t = v_{xx} \\tag{3.1} \\end{equation}\\] If we consider the explicit method for (3.1) we find that \\[\\begin{equation} u_n^{m+1} = \\frac{{\\triangle t}}{{\\triangle x}^2} u_{n+1}^m + \\left(1 - 2 \\frac{{\\triangle t}}{{\\triangle x}^2}\\right) u_n^m + \\frac{{\\triangle t}}{{\\triangle x}^2} u_{n-1}^m \\end{equation}\\] and assuming that \\(\\frac{{\\triangle t}}{{\\triangle x}^2} = 1\\) then \\[\\begin{equation} u_n^{m+1} = u_{n+1}^m - u_n^m + u_{n-1}^m. \\tag{3.2} \\end{equation}\\] Assume the following initial conditions \\[\\begin{equation} v(x,0) = 0 \\end{equation}\\] for all \\(x\\) except \\(\\bar {x}\\) where \\[\\begin{equation} v(\\bar {x},0) = \\varepsilon \\not= 0 \\end{equation}\\] and \\[\\begin{equation} v(a,t) = v(b,t) = 0. \\end{equation}\\] Using equation (3.2) we attain the following table \\[\\begin{equation} \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline {\\rm } &amp; {\\rm } &amp; {\\rm } &amp; {\\rm } &amp; {\\rm } &amp; {\\bar {x}} &amp; {\\rm } &amp; {\\rm } &amp; {\\rm }&amp; {\\rm } &amp; {\\rm }\\\\ \\hline 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\varepsilon &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\varepsilon &amp; -\\varepsilon &amp; \\varepsilon &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\varepsilon &amp; -2\\varepsilon &amp; 3\\varepsilon &amp; -2\\varepsilon &amp; \\varepsilon &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 &amp; 0 \\\\ 0 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\\\ \\varepsilon &amp; -5\\varepsilon &amp; 15\\varepsilon &amp; -30\\varepsilon &amp; 45\\varepsilon &amp; -51\\varepsilon &amp; 45\\varepsilon &amp; -30\\varepsilon &amp; 15\\varepsilon &amp; -5\\varepsilon &amp; \\varepsilon \\\\ \\hline \\end{array} \\tag{3.3} \\end{equation}\\] which clearly indicates how an error can propagate throughout the entire solution. "],
["matrix-method-eigenvalue-analysis.html", "3.1 Matrix method: eigenvalue analysis", " 3.1 Matrix method: eigenvalue analysis When we consider the explicit method we are able to write \\[\\begin{equation} u_n^{m+1} = g_n(u_0^m; u_1^m;...u_N^m) \\end{equation}\\] where \\(g_n(u_0^m; u_1^m;...u_N^m)\\) is some combination of the terms \\(u_0^m; u_1^m;...u_N^m\\). Example 3.1 The partial differential equation \\[\\begin{equation} v_t = v_{xx} \\end{equation}\\] explicitly gives \\[\\begin{equation} u_n^{m+1} = \\frac{{\\triangle t}}{{\\triangle x}^2} u_{n+1}^m + \\left(1 - 2 \\frac{{\\triangle t}}{{\\triangle x}^2}\\right) u_n^m + \\frac{{\\triangle t}}{{\\triangle x}^2} u_{n-1}^m. \\end{equation}\\] Combining these equations for \\(n = 0,1,...,N\\) we find the following system of equations \\[\\begin{equation*} \\left( \\begin{array}{c} u_0^{m+1} \\\\ u_1^{m+1} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_N^{m+1} \\end{array} \\right) \\qquad =\\left( \\begin{array}{c} g_0(u_0^{m}; u_1^{m};...u_N^{m}) \\\\ g_1(u_0^{m}; u_1^{m};...u_N^{m}) \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ g_N(u_0^{m}; u_1^{m};...u_N^{m}). \\end{array} \\right) \\end{equation*}\\] Defining \\(\\underline{u}^m = (u_0^m;...;u_N^m)\\) we can write the system as follows \\[\\begin{equation} \\underline{u}^{m+1} = G(\\underline{u}^m). \\end{equation}\\] To prove stability for a scheme we need to prove that if there exists an error at one level \\(m\\) that this error will not be magnified to produce increasing errors at the levels following \\(m\\), i.e. levels \\(m+1, m+2,...\\). We can do this by considering the error produced at level \\(0\\) and analysing how it propagates. If the partial differential equation is linear this is relatively simple to accomplish since the difference equations will also be linear. So we can write for the explicit scheme \\[\\begin{equation} \\underline{u}^{m+1} = A \\underline{u}^{m} + \\underline{b} \\tag{3.4} \\end{equation}\\] for some matrix \\(A\\) and some constant vector \\(\\underline{b}\\). Similarly for the implicit scheme, if the partial differential equation is linear we get, \\[\\begin{equation} B \\underline{u}^{m+1} = A \\underline{u}^{m} + \\underline{b} \\end{equation}\\] so \\[\\begin{equation} \\underline{u}^{m+1} = (B^{-1}A) \\underline{u}^{m} + (B^{-1}\\underline{b}). \\end{equation}\\] "],
["explicit-scheme.html", "3.2 Explicit Scheme", " 3.2 Explicit Scheme Consider the explicit difference scheme \\[\\begin{equation} u_n^{m+1} = \\beta u_{n+1}^m + (1 - 2 \\beta ) u_n^m + \\beta u_{n-1}^m \\end{equation}\\] where \\(\\beta = \\frac{{\\triangle t}}{{\\triangle x}^2}\\) to form the following system \\[\\begin{equation*} \\left( \\begin{array}{c} u_1^{m+1} \\\\ u_2^{m+1} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N-1}^{m+1} \\end{array} \\right) \\end{equation*}\\] \\[\\begin{equation*} = \\left( \\begin{array}{ccccccc} \\beta &amp; (1-2\\beta ) &amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ 0 &amp; \\beta &amp; (1-2\\beta )&amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\beta&amp; (1-2\\beta )&amp; \\beta &amp; 0\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\beta &amp; (1-2\\beta ) &amp; \\beta \\end{array} \\right) \\qquad \\left( \\begin{array}{c} u_0^{m} \\\\ u_1^{m} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_N^{m} \\end{array} \\right). \\end{equation*}\\] If we include the boundary conditions \\(u_0^m = u_N^m = 0\\) for all \\(m\\) it produces the system \\[\\begin{equation*} \\left( \\begin{array}{c} u_1^{m+1} \\\\ u_2^{m+1} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N-1}^{m+1} \\end{array} \\right) \\end{equation*}\\] \\[\\begin{equation*} = \\left( \\begin{array}{ccccccc} (1-2\\beta ) &amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\beta &amp; (1-2\\beta )&amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots\\\\ \\ldots &amp;\\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\ \\ldots &amp;\\ldots &amp;\\ldots &amp; \\ldots &amp; \\beta &amp; (1-2\\beta )&amp; \\beta\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\beta &amp; (1-2\\beta ) \\end{array} \\right) \\qquad \\left( \\begin{array}{c} u_1^{m} \\\\ u_2^{m} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N-1}^{m} \\end{array} \\right) \\end{equation*}\\] hence \\[\\begin{equation} \\underline{v}^{m+1} = A \\underline{v}^m \\end{equation}\\] where \\(A = I - \\beta C\\) with \\[\\begin{equation*} C = \\left( \\begin{array}{ccccccc} 2 &amp; -1 &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ -1 &amp; 2&amp; -1 &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots\\\\ \\ldots &amp;\\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\ \\ldots &amp;\\ldots &amp;\\ldots &amp; \\ldots &amp; -1 &amp; 2 &amp; -1\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; -1 &amp; 2 \\end{array} \\right). \\end{equation*}\\] Let \\(\\underline{v}^m\\) be the exact solution of the difference equations and \\(\\underline{u}^m\\) be the calculated solution of the difference equations. Define the error vector to be \\[\\underline{e}^m = \\underline{v}^m - \\underline{u}^m.\\] Assume that we make only one initial error \\(\\underline{e}^0\\) then the calculated values are also found from (3.4) \\[\\underline{u}^{m+1} = A \\underline{u}^{m} + \\underline{b}\\] and since \\[ \\underline{v}^{m+1} = A \\underline{v}^{m} + \\underline{b}\\] we find that \\[\\underline{e}^{m+1} = \\underline{v}^{m+1} - \\underline{u}^{m+1} = A \\underline{v}^{m} + \\underline{b} - (A \\underline{u}^{m} + \\underline{b})\\] \\[\\begin{equation} = A \\underline{e}^m. \\tag{3.5} \\end{equation}\\] Assuming that all the eigenvalues of \\(A\\) are distinct we can write the initial error \\[\\begin{equation} \\underline{e}^{0} = \\sum_{n=0}^{N} a_n \\gamma_n \\tag{3.6} \\end{equation}\\] where the \\(a_n\\)’s are some constants and the \\(\\gamma_n\\)’s are the linearly independent eigenvectors corresponding to the eigenvalues \\(\\lambda _n\\). From (3.5) we know that \\(\\underline{e}^m = A^m \\underline{e}^0\\) so from (3.6) we have found that \\[\\underline{e}^m = \\sum_{n=0}^{N} a_n \\lambda _n^m \\gamma_n\\] and so the error vector remains bounded if and only of the eigenvalues of \\(A\\) have absolute value \\(\\leq 1\\), i.e. the method is stable if the eigenvalue, of the matrix, of largest absolute magnitude does not exceed \\(1\\). See Burden &amp; Faires pg 706 - 707 Example 3.2 Consider the matrix discussed above from the explicit scheme \\[\\begin{equation*} A = \\left( \\begin{array}{ccccccc} (1-2\\beta ) &amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\beta &amp; (1-2\\beta )&amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots\\\\ \\ldots &amp;\\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\ \\ldots &amp;\\ldots &amp;\\ldots &amp; \\ldots &amp; \\beta &amp; (1-2\\beta )&amp; \\beta\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\beta &amp; (1-2\\beta ) \\end{array} \\right) \\end{equation*}\\] which is an \\((N-1)\\times (N-1)\\) matrix with eigenvalues \\[\\lambda _n = (1 - 2 \\beta ) + 2 \\sqrt{(\\beta ^2)} \\cos \\left(\\frac{n\\pi }{N} \\right)\\] \\[ = 1-2\\beta +2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right).\\] So for stability we require, for \\(n = 1,2,...,(N-1)\\), such that \\[|1-2\\beta + 2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right)| \\leq 1\\] \\[\\begin{equation} -1\\leq \\left( 1-2\\beta + 2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right) \\right) \\leq 1 \\end{equation}\\] giving that \\[1-2\\beta + 2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right) \\geq -1\\] \\[-2\\beta + 2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right) \\geq -2\\] \\[\\beta \\left(1-\\cos \\left(\\frac{n\\pi }{N} \\right) \\right) \\leq 1\\] \\[\\begin{equation} \\beta \\left(2 \\sin^2 \\left(\\frac{n\\pi }{2N} \\right) \\right) \\leq 1 \\tag{3.7} \\end{equation}\\] for all \\(n\\), and \\[1-2\\beta + 2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right) \\leq 1\\] \\[-2\\beta + 2\\beta \\cos \\left(\\frac{n\\pi }{N} \\right) \\leq 0\\] \\[\\beta \\left(1-\\cos \\left(\\frac{n\\pi }{N} \\right) \\right) \\geq 0\\] \\[\\begin{equation} \\beta \\left(2 \\sin^2 \\left(\\frac{n\\pi }{2N} \\right) \\right) \\geq 0 \\tag{3.8} \\end{equation}\\] for all \\(n\\). Since stability requires (3.7) and (3.8) to hold as \\(N\\rightarrow \\infty\\) the fact that \\[\\lim_{N\\rightarrow \\infty} \\left[\\sin^2 \\left(\\frac{(N-1)\\pi }{2N} \\right) \\right] = 1\\] means that from (3.7) and (3.8) we find that stability will only occur if \\[\\begin{equation} 0 \\leq \\beta \\leq \\frac{1}{2}. \\end{equation}\\] This gives us a necessary and sufficient condition for stability for the method \\(\\underline{u}^{m+1} = A \\underline{u}^{m}.\\) In the next example we consider the case where we have derivative boundary conditions. Remember that earlier it was indicated that we could not determine \\(u_0^1\\) and \\(u_N^1\\) from equation (1.11) since one of the subscripts \\(N+1\\) and \\(-1\\) would reach out of bounds for either of the calculations. It is under such circumstances - where \\(u_0^1\\) and \\(u_N^1\\) cannot be obtained from the usual boundary conditions - that derivative boundary conditions become useful. Example 3.3 Examine the stability of the explicit central difference scheme applied to \\[u_t = u_{xx} \\qquad x \\in [0,1]\\] \\[u_x(0,t) = a_1 (u(0,t) - b_1)\\] \\[\\begin{equation} u_x(1,t) = -a_2 (u(1,t) - b_2) \\end{equation}\\] where \\(a_1, a_2 &gt;0\\). Using \\(u_n^{m+1} = \\beta u_{n+1}^m + (1 - 2 \\beta ) u_n^m + \\beta u_{n-1}^m\\) and \\[\\frac{u_1^m - u_{-1}^m}{2{\\triangle x}} = a_1(u_0^m - b_1) \\qquad a_1&gt;0\\] \\[\\begin{equation} \\frac{u_{N+1}^m - u_{N-1}^m}{2{\\triangle x}} = -a_2(u_N^m - b_2) \\qquad a_2&gt;0 \\end{equation}\\] we obtain for the boundaries \\[u_0^{m+1} = \\beta u_{1}^m + (1 - 2 \\beta ) u_0^m + \\beta u_{-1}^m\\] \\[\\beta (u_1^m - 2ha_1(u_0^m-b_1)) +(1-2 \\beta)u_0^m + \\beta u_1^m\\] \\[\\begin{equation} u_0^m(1-2\\beta-2{\\triangle x}\\beta a_1) + 2 \\beta u_1^m + 2{\\triangle x} \\beta a_1b_1 \\end{equation}\\] and similarly \\[\\begin{equation} u_N^{m+1} = u_{N}^m(1-2\\beta-2{\\triangle x}\\beta a_2) + 2 \\beta u_{N-1}^m + 2{\\triangle x} \\beta a_2b_2. \\end{equation}\\] As such we get the system \\({\\underline u}^{m+1} = A {\\underline u}^m + {\\underline b}\\) where \\[\\begin{equation*} A = \\left( \\begin{array}{ccccccc} (1-2\\beta(1+a_1{\\triangle x}) ) &amp; 2\\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\beta &amp; (1-2\\beta )&amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots\\\\ \\ldots &amp;\\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\ \\ldots &amp;\\ldots &amp;\\ldots &amp; \\ldots &amp; \\beta &amp; (1-2\\beta )&amp; \\beta\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; 2\\beta &amp; (1-2\\beta(1+a_2{\\triangle x}) ) \\end{array} \\right)\\qquad {\\underline b} = \\left( \\begin{array}{c} 2\\beta a_1b_1{\\triangle x} \\\\ 0 \\\\ \\vdots \\\\ \\vdots \\\\ 0 \\\\ 2\\beta a_2b_2{\\triangle x} \\end{array} \\right) \\end{equation*}\\] which means that we only need to look at the matrix since this last vector is constant. The matrix has the form \\(I-\\beta C\\) where \\[\\begin{equation*} C = \\left( \\begin{array}{ccccccc} 2(1+a_1{\\triangle x} ) &amp; -2 &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ -1 &amp; 2&amp; -1 &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots\\\\ \\ldots &amp;\\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp;\\ldots &amp; \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots \\\\ \\ldots &amp;\\ldots &amp;\\ldots &amp; \\ldots &amp; -1 &amp; 2&amp; -1\\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; -2 &amp; 2(1+a_2{\\triangle x} ) \\end{array} \\right) \\end{equation*}\\] of which we need to find the eigenvalues. We will need to use Gerschgorin’s theorem (see your third-year notes) which means that any eigenvalue \\(\\lambda\\) of \\(C\\) satisfies at least one of the following conditions: \\(|\\lambda - 2(1+a_1{\\triangle x})| &lt;2\\), \\(|\\lambda-2|&lt;2\\), \\(|\\lambda - 2(1+a_2{\\triangle x})|&lt;2.\\) This gives three possible intervals for \\(\\lambda\\) but since the eigenvalues of \\(I-\\beta C\\) are \\(1-\\beta \\lambda\\) we require that \\(|1-\\beta \\lambda|&lt;1\\) which means that \\(0&lt;\\beta \\lambda &lt;2\\). But from these intervals we see that \\(\\lambda \\in (0, \\max\\{(4+2a_1{\\triangle x})\\beta;(4+2a_2{\\triangle x}) \\beta\\})\\) so we require that \\(\\max\\{(4+2a_1{\\triangle x})\\beta;(4+2a_2{\\triangle x}) \\beta\\}&lt;2\\) and so for stability we must have that \\[\\begin{equation} \\beta&lt; \\min\\bigg\\lbrace\\frac{1}{2+a_1{\\triangle x}};\\frac{1}{2+a_2{\\triangle x}}\\bigg\\rbrace. \\end{equation}\\] 3.2.1 Tutorial Implement the implicit finite difference method on \\(u_t=u_{xx}\\) with the boundary conditions \\(u(x,t) = u(1,t) = 0\\) and prove that the method is unconditionally stable. "],
["fourier-transform.html", "3.3 Fourier Transform", " 3.3 Fourier Transform When solving initial-value problems on the real line and common analytical tool used to investigate the stability of a scheme is the Fourier transform. If we define the Fourier transform of the independent variable \\(v\\) to be \\[\\begin{equation} \\hat{v}(\\omega,t) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} e^{-i \\omega x} v(x,t) dx \\tag{3.9} \\end{equation}\\] and take the transform of the relevant partial differential equation we obtain an expression which is an ordinary differential equation in transform space reduced from the partial differential equation. The technique is then to solve the ordinary differential equation in transform space and return to our solution space. The method by which we return to our solution space is to use the Fourier inversion formula \\[\\begin{equation} v(x,t) = \\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{\\infty} e^{-i \\omega x} \\hat{v}(\\omega,t) d\\omega. \\tag{3.10} \\end{equation}\\] Example 3.4 Analyse the stability of the difference scheme \\[\\begin{equation} u_n^{m+1} = \\beta u_{n-1}^m + (1-2\\beta)u_n^m + \\beta u_{n+1}^m, \\tag{3.11} \\end{equation}\\] where \\(\\beta = \\nu \\triangle{t}/\\triangle{x}^2\\). Solution: We begin by taking the discrete Fourier transform of both sides of the difference equation. We get \\[\\hat{u}^{m+1}(\\xi) = \\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi} u_n^{m+1}\\] \\[\\hspace{1.5cm} =\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi} (\\beta u_{n-1}^m + (1-2\\beta)u_n^m + \\beta u_{n+1}^m)\\] \\[\\hspace{1.5cm} =\\beta\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi}u_{n-1}^m + (1-2\\beta)\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi}u_{n}^m+ \\beta\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi}u_{n+1}^m\\] \\[\\begin{equation} \\hspace{1.5cm}=\\beta\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi}u_{n-1}^m + (1-2\\beta)\\hat{u}^m(\\xi) + \\beta\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi}u_{n+1}^m. \\tag{3.12} \\end{equation}\\] By making the change of variables \\(g=n\\pm1\\) we get \\[\\frac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n \\xi} u_{n\\pm1}^{m}=\\frac{1}{\\sqrt{2 \\pi}}\\sum_{g=-\\infty}^{\\infty} e^{-i (g\\mp1) \\xi} u_g^{m}\\] \\[\\hspace{2.5cm} =e^{\\pm i \\xi} \\frac{1}{\\sqrt{2 \\pi}}\\sum_{g=-\\infty}^{\\infty} e^{-i g \\xi} u_g^{m}\\] \\[\\begin{equation} \\hspace{1.5cm} =e^{\\pm i \\xi} \\hat{u}^m(\\xi). \\tag{3.13} \\end{equation}\\] Now, using expression (3.13) and going back to (3.12) leads us to \\[\\hat{u}^{m+1}(\\xi) = \\beta e^{-i \\xi} \\hat{u}^m(\\xi) + (1-2\\beta)\\hat{u}^m(\\xi) + \\beta e^{i \\xi} \\hat{u}^m(\\xi)\\] \\[\\hspace{1cm} =(\\beta e^{-i \\xi} + (1-2\\beta) + \\beta e^{i \\xi}) \\hat{u}^m(\\xi)\\] \\[\\hspace{1cm} =(2\\beta \\cos \\xi + (1-2\\beta))) \\hat{u}^m(\\xi)\\] \\[\\begin{equation} =\\left(1-4\\beta \\sin^2 \\frac{\\xi}{2} \\right) \\hat{u}^m(\\xi). \\tag{3.14} \\end{equation}\\] The coefficient of \\(\\hat{u}^m\\) in equation (3.14) \\[\\begin{equation} \\rho(\\xi) = 1-4\\beta \\sin^2 \\frac{\\xi}{2} \\tag{3.15} \\end{equation}\\] is called the symbol of the difference scheme (3.11). Thus we see that taking the discrete Fourier transform gets rid of the \\(x\\) derivatives and greatly simplifies the equation. If we apply the result of (3.14) \\(m+1\\) times we get \\[\\begin{equation} \\hat{u}^{m+1}(\\xi) = \\left(1-4\\beta \\sin^2 \\frac{\\xi}{2} \\right)^{m+1} \\hat{u}^0(\\xi). \\tag{3.16} \\end{equation}\\] If we restrict \\(\\beta\\) so that \\[\\begin{equation} |1-4\\beta \\sin^2 \\frac{\\xi}{2}|\\leq1 \\tag{3.17} \\end{equation}\\] then we know that the difference scheme is stable whenever (3.17) is satisfied. The inequality (3.17) is equivalent to \\[\\begin{equation} -1\\leq 1-4\\beta \\sin^2 \\frac{\\xi}{2}\\leq 1 \\tag{3.18} \\end{equation}\\] and as such we need the following to be maintained \\[\\begin{equation} 1-4\\beta \\sin^2 \\frac{\\xi}{2}\\leq 1 \\tag{3.19} \\end{equation}\\] which is always true and \\[\\begin{equation} -1\\leq 1-4\\beta \\sin^2 \\frac{\\xi}{2} \\tag{3.20} \\end{equation}\\] which means that \\(2 \\geq 4\\beta \\sin^2 \\frac{\\xi}{2}\\) which holds when \\(\\beta\\leq 1/2\\). Thus \\(\\beta\\leq 1/2\\) is a sufficient condition for stability (and along with consistency, for convergence too). "],
["von-neumann-stability-analysis.html", "3.4 Von Neumann Stability Analysis", " 3.4 Von Neumann Stability Analysis Errors will propagate through the solution \\(e^m = A^m e^0\\). We can bound the errors by the equivalent conditions \\(||A||\\leq1\\) or \\(\\text{max}_s |\\lambda_s|\\leq1\\). Using the norm condition, we find that the explicit scheme requires \\(\\beta\\leq\\frac{1}{2}\\) and from an eigenvalue analysis the Crank-Nicolson scheme has no restrictions. The condition \\(||A||\\leq1\\) does not make allowance for solutions of the partial differential equation which may be growing exponentially in time. A necessary and sufficient condition for stability when the solution of the partial differential equation is increasing exponentially in time is \\(||A||\\leq 1+B\\triangle{t} = 1 + \\mathcal{O}(\\triangle{t})\\) where \\(B\\) is a constant independent of \\(\\triangle{x}\\) and \\(\\triangle{t}\\). As such a very versatile tool for analysing stability is the Fourier method developed by von Neumann. Here initial values at mesh points are expressed in terms of a finite Fourier series, and we consider the growth of individual Fourier components. We do not need to find eigenvalues, or matrix norms. When we apply the von Neumann analysis we require \\(|g|\\leq1\\) where \\(g\\) is termed the amplification factor. If the exact solution of the partial differential equation grows exponentially, then the difference scheme will allow such solutions if \\(|g|\\leq 1 + B\\triangle{t}\\) where \\(B\\) is independent of \\(\\triangle{x}\\) and \\(\\triangle{t}\\). Example 3.5 Consider the following \\(u_t = \\kappa u_{xx}\\) as an implicit scheme \\[\\begin{equation} \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}} = \\kappa \\frac{u_{n+1}^{m+1} - 2 u_n^{m+1} + u_{n-1}^{m+1}}{\\triangle{x}^2}. \\tag{3.21} \\end{equation}\\] We now substitute \\(u_n^m = U^m e^{I \\omega n \\triangle{x}}\\) where \\(I^2 = -1\\) into (3.21) to obtain \\[\\begin{equation} \\frac{1}{\\triangle{t}} U^m (U-1)e^{I \\omega n \\triangle{x}} = \\frac{\\kappa U^{m+1}}{\\triangle{x}^2} \\left(e^{-I \\omega \\triangle{x}} - 2 + e^{I \\omega \\triangle{x}} \\right) e^{I \\omega n \\triangle{x}}. \\tag{3.22} \\end{equation}\\] where \\(e^{-I \\omega \\triangle{x}} - 2 + e^{I \\omega \\triangle{x}} = 2(\\cos(\\omega \\triangle{x}) - 1)\\). With \\(\\beta = \\triangle{t} \\kappa/\\triangle{x}^2\\) we get \\[\\begin{equation} U-1 = \\beta U (2 \\cos(\\omega \\triangle{x})-2) = -4 \\beta U \\sin^2 \\left(\\frac{\\omega \\triangle{x}}{2}\\right). \\tag{3.23} \\end{equation}\\] This gives \\(g = \\frac{U^{m+1}}{U^m} = \\frac{1}{1+4\\beta \\sin^2\\left(\\frac{\\omega \\triangle{x}}{2} \\right)}\\) and thus \\(0\\leq g\\leq 1\\) for all \\(\\beta&gt;0\\) and for all \\(\\omega\\). Thus the fully implicit scheme is unconditionally stable. 3.4.1 Tutorial Consider the following difference scheme and conduct a von Neumann stability analysis to determine its stability: \\[\\frac{u_n^{m+1} - u_n^{m-1}}{2\\triangle{t}} = \\kappa \\left[\\frac{u_{n+1}^m - 2 u_n^m + u_{n-1}^m}{\\triangle{x}^2}\\right].\\] "],
["finite-difference-schemes-for-parabolic-pdes.html", "Chapter 4 Finite Difference Schemes for Parabolic PDEs", " Chapter 4 Finite Difference Schemes for Parabolic PDEs We define the following notation: \\[\\delta_{+} u_n = u_{n+1} - u_n\\] \\[\\delta_{-} u_n = u_n - u_{n-1}\\] \\[\\delta_0 u_n = u_{n+1} - u_{n-1}\\] \\[\\delta^2 u_n = u_{n+1} - 2 u_n + u_{n-1}\\] The stepsizes can be defined in different ways as follows: \\(dx = \\triangle{x} = h\\) and \\(dt = \\triangle{t} = k\\). "],
["standard-finite-difference-schemes-for-parabolic-pdes.html", "4.1 Standard Finite Difference Schemes for Parabolic PDEs", " 4.1 Standard Finite Difference Schemes for Parabolic PDEs The form for a parabolic partial differential equation, i.e. heat or diffusion equation, is often given as follows, \\[\\begin{equation} v_t = \\alpha ^2 v_{xx}, \\quad 0&lt;x&lt;l, \\quad t&gt;0 \\end{equation}\\] where \\(v_t = \\frac{\\partial v(x,t)}{\\partial t}\\) and \\(v_{xx} = \\frac{\\partial ^2v(x,t)}{\\partial x^2}\\), subject to the conditions \\[\\begin{equation} v(0,t) = v(l,t) = 0, \\quad t&gt;0, \\end{equation}\\] \\[\\begin{equation} v(x,0) = f(0), \\quad 0\\leq x\\leq l, \\end{equation}\\] where \\(v(x, t)\\) is the unknown function to be solved for, with \\(x\\) a coordinate in space and \\(t\\) a temporal coordinate. The coefficient \\(\\alpha\\) is the diffusion coefficient and determines how fast \\(v\\) changes in time. Compared to the wave equation, \\(v_{tt} = c^2v_{xx}\\), which looks quite similar, however, the heat equation exhibits solutions very different from those of the wave equation. Additionally, it has significantly different demands to the numerical methods utilised. Usually, diffusion problems often experience a rapid change in solution in beginning, but over time, the evolution of \\(v\\) becomes slower and slower. Generally, the solutions to the heat equation are smooth, and over time will become indistinguishable to the shape of the initial condition. This is in shark contrast to the wave equation, in which case the initial condition is preserved, with the solution is in essence simply this moving initial condition. The wave equation has solutions which propagate with speed \\(c\\) indefinitely, while keeping its shape. The heat equation meanwhile, converges to a steady solution \\(\\bar{u}(x)\\) as \\(t \\rightarrow \\infty\\). The difference scheme is obtained by replacing all derivatives by finite difference approximations at the nodes of the grid. The idea is to calculate the values of \\(u(x,t)\\) for all \\(x\\) values for a given fixed value of \\(t\\). Assuming that the values \\[u_n^{m},\\ n = 0,1,2,....,N\\] are known we use the difference scheme to calculate the new row \\[u_n^{m+1},\\ n = 0,1,2,....,N.\\] If the difference scheme involves only one point \\(u_n^{m+1}\\) at the \\(t\\)-level \\(m+1\\) (hence we calculate \\(u_n^{m+1}\\) for each \\(n = 0,1,2,....,N\\)) without having to know any other \\(u\\) values on the \\(t\\)-level \\(m+1\\) then the method is called explicit. FIGURE 4.1: Explicit Scheme Stencil Example 4.1 Consider \\[\\begin{equation} v_t = v_{xx} \\end{equation}\\] Approximate \\(v_t\\) using the forward difference approximation at \\(t_m\\) giving \\[\\begin{equation} v_t |_{(x_n,t_m)} = \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}} \\end{equation}\\] and \\(v_{xx}\\) using the central difference approximation at \\(x_n\\) giving \\[\\begin{equation} v_{xx} |_{(x_n,t_m)} = \\frac{u_{n+1}^m - 2 u_n^m + u_{n-1}^m}{\\triangle{x}^2} \\end{equation}\\] Equating these equations as per the example we find that \\[\\begin{equation} u_n^{m+1} = \\frac{\\triangle{t}}{\\triangle{x}^2} \\left( u_{n+1}^m - 2 u_n^m + u_{n-1}^m \\right) + u_n^m. \\tag{4.1} \\end{equation}\\] Since we know the values of \\(u\\) on the \\(m\\)-level we are able to obtain the values of \\(u\\) on the \\((m+1)\\)-level. The values \\(u_0^{m+1}\\) and \\(u_N^{m+1}\\) are found using the boundary conditions - they can not be found using the scheme given above. The general computational algorithm for the explicit scheme is: Compute \\(u^0_n = f(x_n)\\) for \\(n = 0, \\ldots, N_x\\) For \\(m = 0, \\ldots, N_t\\): Apply scheme such as Equation (4.1) for all the internal spatial points \\(n = 1, \\ldots, N_x - 1\\) Set the boundary values \\(u_n^{m+1}\\) = BC for \\(n = 0\\) and \\(n = N_x\\) Example 4.2 Let us again consider \\[\\begin{equation} v_t = v_{xx} \\end{equation}\\] with boundary conditions \\[\\begin{equation} v(0,t) = 0 = v(1,t) \\qquad \\text{for all $t \\geq 0$} \\end{equation}\\] and starting values \\[\\begin{equation*} v(x,0) = \\left\\{ \\begin{array}{rl} 2x &amp; \\text{if } x \\in [0,\\frac{1}{2}]\\\\ 2(1-x) &amp; \\text{if } x \\in [\\frac{1}{2},1] \\end{array} \\right. \\end{equation*}\\] Given that there is a symmetry around \\(x=1/2\\) we only need to calculate solutions to \\(u\\) for \\(x\\in [0,\\frac{1}{2}]\\). We are given that \\(u_0^m = u_N^m = 0\\) for all \\(m\\). Let us suppose that \\(\\beta = \\frac{\\triangle{t}}{\\triangle{x}^2} = \\frac{1}{10}\\) and \\(\\triangle{x} = \\frac{1}{10}\\) then we know that \\(\\triangle{t} = \\frac{1}{1000}\\). We need to calculate \\(x=0.1, 0.2, 0.3, 0.4\\) and \\(0.5\\). Thus \\[u_n^{m+1} = \\beta u_{n+1}^m + (1- 2\\beta) u_n^m + \\beta u_{n-1}^m\\] \\[\\begin{equation} =\\frac{1}{10}u_{n+1}^m +\\frac{8}{10} u_n^m + \\frac{1}{10} u_{n-1}^m. \\end{equation}\\] We can structure Table 4.1 where the first row of values is the given starting values and the first column of values is the given boundary conditions. TABLE 4.1: Table indicating the propagation of error. x 0 0.1 0.2 0.3 0.4 0.5 t 0 0 0.2 0.4 0.6 0.8 1 0.001 0 0.2 0.4 0.6 0.8 0.96 0.002 0 0.2 0.4 0.6 0.796 0.928 ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ Here we see the Python result and tabulated results of Example 4.2. ## x = 0.0 x = 0.1 x = 0.2 x = 0.3 x = 0.4 x = 0.5 ## t = 0.0 0.0 0.2 0.40000 0.60000 0.8000 1.0000 ## t = 0.001 0.0 0.2 0.40000 0.60000 0.8000 0.9600 ## t = 0.002 0.0 0.2 0.40000 0.60000 0.7960 0.9280 ## t = 0.003 0.0 0.2 0.40000 0.59960 0.7896 0.9016 ## t = 0.004 0.0 0.2 0.39996 0.59864 0.7818 0.8792 4.1.1 Higher and Mixed Derivatives Using the Taylor series, we can derive the weights for higher derivatives. Some of these are listed in Tables 4.2, 4.3 and 4.4. TABLE 4.2: Forward Difference Quotient \\(\\mathcal{O}(\\Delta x)\\) \\(u_i\\) \\(u_{i + 1}\\) \\(u_{i+2}\\) \\(u_{i + 3}\\) \\(u_{i+4}\\) \\(\\Delta x \\dfrac{\\partial u}{\\partial x}\\) -1 1 \\(\\Delta x^2 \\dfrac{\\partial^2 u}{\\partial x^2}\\) 1 -2 1 \\(\\Delta x^3 \\dfrac{\\partial^3 u}{\\partial x^3}\\) -1 3 -3 1 \\(\\Delta x^4 \\dfrac{\\partial^4 u}{\\partial x^4}\\) 1 -4 6 -4 1 TABLE 4.3: Backward Difference Quotient \\(\\mathcal{O}(\\Delta x)\\) \\(u_{i-4}\\) \\(u_{i-3}\\) \\(u_{i-2}\\) \\(u_{i -1}\\) \\(u_{i}\\) \\(\\Delta x \\dfrac{\\partial u}{\\partial x}\\) -1 1 \\(\\Delta x^2 \\dfrac{\\partial^2 u}{\\partial x^2}\\) 1 -2 1 \\(\\Delta x^3 \\dfrac{\\partial^3 u}{\\partial x^3}\\) -1 3 -3 1 \\(\\Delta x^4 \\dfrac{\\partial^4 u}{\\partial x^4}\\) 1 -4 6 -4 1 TABLE 4.4: Central Difference Quotient \\(\\mathcal{O}(\\Delta x^2)\\) \\(u_{i-2}\\) \\(u_{i-1}\\) \\(u_{i}\\) \\(u_{i +1}\\) \\(u_{i+2}\\) \\(\\Delta x \\dfrac{\\partial u}{\\partial x}\\) -1 0 1 \\(\\Delta x^2 \\dfrac{\\partial^2 u}{\\partial x^2}\\) 1 -2 1 \\(\\Delta x^3 \\dfrac{\\partial^3 u}{\\partial x^3}\\) -1 2 0 -2 1 \\(\\Delta x^4 \\dfrac{\\partial^4 u}{\\partial x^4}\\) 1 -4 6 -4 1 Finite difference approximations for mixed partial derivatives can be calculated in much the same way. For example, let us compute the central approximation for the derivative \\(\\partial^2 u/\\partial x \\partial y\\): \\[\\begin{eqnarray} &amp;&amp;\\frac{\\partial^{2} u}{\\partial x \\partial y}=\\frac{\\partial}{\\partial x}\\left(\\frac{\\partial u}{\\partial y}\\right)=\\frac{\\partial}{\\partial x}\\left(\\frac{u(x, y+\\Delta y)-u(x, y-\\Delta y)}{2 \\Delta y}+\\mathcal{O}\\left(\\Delta y^{2}\\right)\\right)= \\nonumber \\\\ &amp;&amp; \\dfrac{u(x+\\triangle x, y+\\triangle y)-u(x-\\triangle x, y+\\triangle y)-u(x+\\triangle x, y-\\triangle y)+u(x-\\triangle x, y-\\Delta y)}{4\\Delta x\\Delta y} \\nonumber \\\\ &amp;&amp;+\\mathscr{O}\\left(\\triangle x^{2} \\triangle y^{2}\\right) \\end{eqnarray}\\] 4.1.2 Higher Order Approximations We can also find higher order approximations to any derivative. Recall from the 2nd year Numerical Analysis course, that we can use the Taylor series to obtain any approximation of our choosing. Example 4.3 Say we whish to derive the second order accurate first derivative approximation using the Taylor series and method of undetermined coefficients. This requires the values \\((x_{i+1}, u(x_{i+1}))\\) and \\((x_{i-1}, u(x_{i-1}))\\). We can express these values using the Taylor series approximation: \\[\\begin{eqnarray} u\\left(x_{n+1}\\right)&amp;=&amp;u\\left(x_{n}\\right)+\\left(x_{n+1}-x_{n}\\right) u^{\\prime}\\left(x_{n}\\right)+\\frac{\\left(x_{n+1}-x_{n}\\right)^{2}}{2 !} u^{\\prime \\prime}\\left(x_{n}\\right)+\\frac{\\left(x_{n+1}-x_{n}\\right)^{3}}{3 !} u^{\\prime \\prime \\prime}\\left(x_{n}\\right) \\nonumber \\\\ &amp;&amp; +\\mathcal{O}\\left(\\left(x_{n+1}-x_{n}\\right)^{4}\\right) \\nonumber \\\\ &amp;=&amp; u\\left(x_{n}\\right)+\\Delta x u^{\\prime}\\left(x_{n}\\right)+\\frac{\\Delta x^{2}}{2 !} u^{\\prime \\prime}\\left(x_{n}\\right)+\\frac{\\Delta x^{3}}{3 !} u^{\\prime \\prime \\prime}\\left(x_{n}\\right)+\\mathcal{O}\\left(\\Delta x^{4}\\right) \\end{eqnarray}\\] and: \\[\\begin{eqnarray} u\\left(x_{n-1}\\right)&amp;=&amp;u\\left(x_{n}\\right)+\\left(x_{n-1}-x_{n}\\right) u^{\\prime}\\left(x_{n}\\right)+\\frac{\\left(x_{n-1}-x_{n}\\right)^{2}}{2 !} u^{\\prime \\prime}\\left(x_{n}\\right)+\\frac{\\left(x_{n-1}-x_{n}\\right)^{3}}{3 !} u^{\\prime \\prime \\prime}\\left(x_{n}\\right) \\nonumber \\\\ &amp;&amp; +\\mathcal{O}\\left(\\left(x_{n-1}-x_{n}\\right)^{4}\\right) \\nonumber \\\\ &amp;=&amp; u\\left(x_{n}\\right)-\\Delta x u^{\\prime}\\left(x_{n}\\right)+\\frac{\\Delta x^{2}}{2 !} u^{\\prime \\prime}\\left(x_{n}\\right)-\\frac{\\Delta x^{3}}{3 !} f^{\\prime \\prime \\prime}\\left(x_{n}\\right)+\\mathcal{O}\\left(\\Delta x^{4}\\right) \\end{eqnarray}\\] Next we need a representation of what our assumed approximation looks like. In this case it is: \\[\\begin{equation} u^{\\prime}\\left(x_{n}\\right)+R\\left(x_{n}\\right)=A u\\left(x_{n+1}\\right)+B u\\left(x_{n}\\right)+C u\\left(x_{n-1}\\right),\\tag{4.2} \\end{equation}\\] where \\(R(x_n)\\) is the associated error with our approximation. We now plug the Taylor series approximations we found into Equation (4.2): \\[\\begin{eqnarray} &amp;&amp;u^{\\prime}\\left(x_{n}\\right)+R\\left(x_{n}\\right)=A\\left(u\\left(x_{n}\\right)+\\Delta x u^{\\prime}\\left(x_{n}\\right)+\\frac{\\Delta x^{2}}{2 !} u^{\\prime \\prime}\\left(x_{n}\\right)+\\frac{\\Delta x^{3}}{3 !} u^{\\prime \\prime \\prime}\\left(x_{n}\\right)+\\mathcal{O}\\left(\\Delta x^{4}\\right)\\right)\\nonumber \\\\ &amp;&amp;+B u\\left(x_{n}\\right)+C\\left(u\\left(x_{n}\\right)-\\Delta x u^{\\prime}\\left(x_{n}\\right)+\\frac{\\Delta x^{2}}{2 !} u^{\\prime \\prime}\\left(x_{n}\\right)-\\frac{\\Delta x^{3}}{3 !} u^{\\prime \\prime \\prime}\\left(x_{n}\\right) +\\mathcal{O}\\left(\\Delta x^{4}\\right)\\right). \\end{eqnarray}\\] Recall that we are deriving a second order approximation, and thus we require \\(R(x_n) = \\mathcal{O}(\\Delta x^2)\\). This means that we want all lower terms than this to vanish, except for those terms which multiply \\(u^\\prime(x_n)\\) which need to sum to 1 in order to give us the approximation. Therefore, collecting terms over common derivatives we get: \\[\\begin{eqnarray} u(x_n) &amp;:&amp; A + B + C = 0,\\nonumber \\\\ u^\\prime(x_n) &amp;:&amp; A\\Delta x - C\\Delta x = 1,\\nonumber \\\\ u^{\\prime\\prime}(x_n) &amp;:&amp; A\\dfrac{\\Delta x^2}{2} + C\\dfrac{\\Delta x^2}{2} = 0.\\nonumber \\end{eqnarray}\\] Solving the third equation shows that \\(A = -C\\), which when substituted into the second equation gives \\(A = \\frac{1}{2\\Delta x}\\) and \\(C = -\\frac{1}{2\\Delta x}\\). Finally, solving for the third equation implies that \\(B = 0\\). Substituting back into Equation (4.2) we have: \\[ u^\\prime(x_{n}) = \\dfrac{u(x_{n + 1}) - u(x_{n - 1})}{2\\Delta x}. \\] Exercise 4.1 Repeat the approach done in Example 4.3 to find the second order accurate second derivative formula (i.e. central). 4.1.3 General Approach We may roughly generalise the above procedure for a finite difference approximation located at \\(\\bar{x}\\) to the \\(k-th\\) derivative using an arbitrary stencil \\(N \\geq k + 1\\) points \\(x_1, \\ldots, x_N\\). Again at each stencil point we have the approximation: \\[ u\\left(x_{i}\\right)=u(\\overline{x})+\\left(x_{i}-\\overline{x}\\right) u^{\\prime}(\\overline{x})+\\cdots+\\frac{1}{k !}\\left(x_{i}-\\overline{x}\\right)^{k} u^{(k)}(\\overline{x})+\\cdots \\] From the above approach, we are searching for the linear combination of these Taylor expansions such that: \\[ u^{(k)}(\\overline{x})+\\mathcal{O}\\left(\\Delta x^{p}\\right)=a_{1} u\\left(x_{1}\\right)+a_{2} u\\left(x_{2}\\right)+a_{3} u\\left(x_{3}\\right)+\\cdots+a_{n} u\\left(x_{n}\\right). \\] Using the above along with the method of undermined coefficients, we need to eliminate the terms of the above approximation that are in front of the derivatives less than order \\(k\\). The mathematical condition for this is: \\[ \\frac{1}{(i-1) !} \\sum_{j=1}^{N} a_{j}\\left(x_{j}-\\overline{x}\\right)^{(i-1)}=\\left\\{\\begin{array}{ll}{1} &amp; {\\text { if } \\quad i-1=k,} \\\\ {0} &amp; {\\text { otherwise, }}\\end{array}\\right. \\] for \\(i = 1,\\ldots, N\\). We can now write these into a system of equations which will have a unique solution. Example 4.4 Coding up the above strategy, let us test some examples. Inputting a first and second derivative, both centered around 0 and using three points (i.e. central), we get: print(finite_difference(1, 0.0, np.asarray([-1.0, 0.0, 1.0]))) ## [-0.5 0. 0.5] print(finite_difference(2, 0.0, np.asarray([-1.0, 0.0, 1.0]))) ## [ 1. -2. 1.] which yield our standard central first and second derivatives. Finally, let us test to see if we can complete Table 4.4. print(finite_difference(4, 0.0, np.asarray([-2.0, -1.0, 0.0, 1.0, 2.0]))) ## [ 1. -4. 6. -4. 1.] Note: In the literature, it is more common that higher ordered schemes and their associated weights are generated via Fornberg’s algorithm. You can read his original paper here. Exercise 4.2 Write a function which implements Fornberg’s algorithm. Use your program to recreate Tables 1 - 4 from the paper linked above. 4.1.4 Tutorial Prove that the functions (i) \\(u(x, t) = e^{2t+x} + e^{2t-x}\\), (ii) \\(u(x, t) = e^{2t+x}\\) are solutions of the heat \\(u_t = u_{xx}\\) with the specified initial boundary conditions: \\[ (i) \\begin{cases} u(x, 0) = 2\\cosh x\\quad \\forall \\quad 0 \\leq x \\leq 1\\\\ u(0, t) = 2e^{2t} \\quad \\forall \\quad 0 \\leq t \\leq 1\\\\ u(1, t) = (e^2 + 1)e^{2t - 1} \\quad \\forall \\quad 0 \\leq t \\leq 1 \\end{cases} \\qquad (ii) \\begin{cases} u(x, 0) = e^ x\\quad \\forall \\quad 0 \\leq x \\leq 1\\\\ u(0, t) = e^{2t} \\quad \\forall \\quad 0 \\leq t \\leq 1\\\\ u(1, t) = e^{2t - 1} \\quad \\forall \\quad 0 \\leq t \\leq 1 \\end{cases} \\] Prove that if \\(f(x)\\) is a degree 3 polynomial, then \\(u(x, t) = f(x) + ct f^{\\prime\\prime\\prime}(x)\\) is a solution of the initial value problem \\(u_t = cu_{xx}, u(x, 0) = f(x)\\). Prove that the functions (i) \\(u(x, t) = e^{-\\pi t }\\sin \\pi x\\), (ii) \\(u(x, t) = e^{-\\pi t }\\cos \\pi x\\) are solutions of the heat \\(\\pi u_t = u_{xx}\\) with the specified initial boundary conditions: \\[ (i) \\begin{cases} u(x, 0) = \\sin\\pi x\\quad \\forall \\quad 0 \\leq x \\leq 1\\\\ u(0, t) = 0 \\quad \\forall \\quad 0 \\leq t \\leq 1\\\\ u(1, t) = 0 \\quad \\forall \\quad 0 \\leq t \\leq 1 \\end{cases} \\qquad (ii) \\begin{cases} u(x, 0) = \\cos\\pi x\\quad \\forall \\quad 0 \\leq x \\leq 1\\\\ u(0, t) = e^{-\\pi t} \\quad \\forall \\quad 0 \\leq t \\leq 1\\\\ u(1, t) = -e^{-\\pi t} \\quad \\forall \\quad 0 \\leq t \\leq 1 \\end{cases} \\] Conduct a von Neumann stability analysis on the following scheme: \\[ u_n^{m + 1} = u_n^m + \\dfrac{D\\Delta t}{\\Delta x^2}\\left(u_{n+1}^m - 2u_n^m + u_{n-1}^m\\right). \\] "],
["sec-this.html", "4.2 Derivative Boundary Conditions", " 4.2 Derivative Boundary Conditions Under these circumstances the boundary conditions are given in terms of derivatives at the boundary and we need extra equations to calculate \\(u\\) on the boundary. The boundary condition can either be approximated by a forward (or backward) difference approximation - depending on whether the derivative boundary condition is given at the start or end of the interval - or by using a central difference approximation. Forward/Backward Difference: If it is given that \\(v_{x} |_{(0,t)} = \\alpha\\) then \\[\\frac{u(\\triangle{x},t) - u(0,t)}{\\triangle{x}} = \\alpha\\] \\[u(0,t) = u(\\triangle{x},t) - \\triangle{x}\\alpha \\] giving that \\[u_0^{m+1} = u_1^{m+1} - \\triangle{x} \\alpha.\\] This is done in a similar fashion if \\(v_{x} |_{(1,t)}\\) is given. Central Difference: If it is given that \\(v_{x} |_{(0,t)} = \\alpha\\) then \\[\\frac{u(\\triangle{x},t) - u(-\\triangle{x},t)}{2\\triangle{x}} = \\alpha\\] \\[u(-\\triangle{x},t) = u(\\triangle{x},t) - 2\\triangle{x}\\alpha\\] giving that \\[u_{-1}^{m+1} = u_1^{m+1} - 2\\triangle{x} \\alpha.\\] It is important to note that the point \\(u(-\\triangle{x},t)\\) is fictitious and falls outside the region of interest. The last equation is combined with the difference approximation to the partial differential equation applied at \\((0,t)\\) which will eliminate the term \\(u_{-1}^{m}\\). Example 4.5 Approximate \\(v_t = v_{xx}\\) by the explicit formula to obtain \\[\\begin{equation} u_n^{m+1} = \\beta u_{n+1}^m + (1- 2\\beta) u_n^m + \\beta u_{n-1}^m \\end{equation}\\] where \\(\\beta = \\triangle{t}/\\triangle{x}^2\\) and \\(v_x(0,t)\\) is given. To find \\(u_0^{m+1}\\) the formula gives \\[\\begin{equation} u_0^{m+1} = \\beta u_{1}^m + (1- 2\\beta) u_0^m + \\beta u_{-1}^m. \\end{equation}\\] We may assume that \\(u_0^m\\) and \\(u_1^m\\) are known due to boundary conditions. However we need to eliminate \\(u_{-1}^m\\) which we do through the use of the derivative condition \\[\\begin{equation} \\frac{u_1^m - u_{-1}^m}{2 \\triangle{x}} = d \\end{equation}\\] Which means that \\(u_{-1}^m = u_1^m - 2hd\\) and as such \\[u_0^{m+1} = \\beta u_{1}^m + (1- 2\\beta) u_0^m + \\beta (u_1^m - 2hd)\\] \\[\\begin{equation} = 2\\beta u_{1}^m + (1- 2\\beta) u_0^m - 2\\beta hd. \\end{equation}\\] Example 4.6 Consider \\(v_t = v_{xx}\\) for \\(x \\in [0,1]\\) where \\[v(x,0)=1 \\qquad \\forall \\qquad x \\in [0,1]\\] \\[ v_{x} |_{(0,t)} = v(0,t) \\qquad \\forall t\\geq 0\\] \\[\\begin{equation} v_{x} |_{(1,t)} = -v(1,t) \\qquad \\forall t\\geq 0. \\end{equation}\\] Using the explicit finite difference form as before we find that \\[\\begin{equation} u_n^{m+1} = \\beta u_{n+1}^m + (1- 2\\beta) u_n^m + \\beta u_{n-1}^m \\end{equation}\\] and approximating the derivative boundary conditions by a central difference approximation we obtain \\[\\frac{u_1^m - u_{-1}^m}{2 \\triangle{x}} = u_0^m\\] \\[\\begin{equation} \\frac{u_{N+1}^m - u_{N-1}^m}{2 \\triangle{x}} = u_N^m \\end{equation}\\] which gives \\[{\\underline{n=0 }} \\qquad u_0^{m+1} = 2\\beta u_{1}^m + (1- 2\\beta - 2 \\beta \\triangle{x}) u_0^m\\] \\[\\begin{equation} {\\underline{n=N }} \\qquad u_N^{m+1} = 2\\beta u_{N-1}^m + (1- 2\\beta + 2 \\beta \\triangle{x}) u_N^m. \\end{equation}\\] As an exercise solve this problem numerically for different values of \\(t_M\\), \\(\\triangle{t}\\) and \\(\\triangle{x}\\). "],
["implicit-methods.html", "4.3 Implicit methods", " 4.3 Implicit methods These methods are constructed in such a way that two or more of the unknown values \\(u_n^{m+1}\\) are specified in terms of the know values in the row \\(u_n^m\\) in one scheme. Hence to calculate the \\(n+1\\) unknown values \\[u_0^{m+1}, u_1^{m+1}, u_2^{m+1},......, u_N^{m+1}\\] we need to set up \\(N+1\\) equations and solve them simultaneously. Discretisation methods that lead to a coupled system of equations for the unknown function at a new time level are said to be implicit methods. When the discretisation method can yield a simple explicit formula for the values of the unknown function at each spatial point at the next time step, then we are utilising explicit methods. Consider the example \\[v_t = v_{xx}\\] where we use the backward and central difference approximation to find that \\[\\begin{equation} \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}} = \\frac{u_{n+1}^{m+1} - 2 u_n^{m+1} + u_{n-1}^{m+1}}{\\triangle{x}^2} \\end{equation}\\] giving \\[\\begin{equation} -\\beta u_{n-1}^{m+1} + (1+2\\beta )u_n^{m+1} - \\beta u_{n+1}^{m+1} = u_n^m \\end{equation}\\] where \\(\\beta = \\frac{\\triangle{t}}{\\triangle{x}^2}\\). Hence assuming we are given the boundary conditions \\(u_0^{m+1} = v(0,t)\\) and \\(u_N^{m+1} = v(1,t)\\) we need to solve the following \\(N-1\\) linear equations \\[\\begin{equation*} \\left( \\begin{array}{cccccc} (1+2\\beta ) &amp; -\\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ -\\beta &amp; (1+2\\beta )&amp; -\\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; -\\beta &amp; (1+2\\beta ) &amp; -\\beta \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; -\\beta &amp; (1+2\\beta ) \\end{array} \\right) \\qquad \\left( \\begin{array}{c} u_1^{m+1} \\\\ u_2^{m+1} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N-1}^{m+1} \\end{array} \\right) \\end{equation*}\\] \\[\\begin{equation} = \\left( \\begin{array}{c} u_1^{m} + \\beta u_0^{m+1} \\\\ u_2^{m} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N-1}^{m} + \\beta u_N^{m+1} \\end{array} \\right). \\tag{4.3} \\end{equation}\\] Some of the equations must be modified if the boundary conditions include derivative terms as referred to in Section 4.2. FIGURE 4.2: Implicit Scheme Stencil Example 4.7 Solve the heat equation using a finite difference implicit scheme which is backward difference in time and central difference in space. Do so by deriving all necessary components by hand, as well as implementing your solution in Python. \\[ u_{t} = Du_{xx},\\ \\ \\quad u(0, t) = \\alpha, \\ \\ u(1, t) = \\beta,\\ \\ \\qquad u(x, 0) = f(x) \\] Given: \\(dt\\): Time step \\(dx\\): Spatial step \\(M\\): Number of iterations in the temporal space ie. number of time steps \\(D\\): Constant coefficient \\(f(x)\\): Initial condition function \\(\\alpha\\): Boundary Value at \\(x_{n=0}\\) \\(\\beta\\): Boundary Value \\(x_{n=N}\\) #GIVEN D = 1 dx = 0.2 dt = (dx**2)/(4*D) M = 4 alpha = 0 beta = 0 f = lambda x: np.exp(-100*(x - 0.5)**2) Finite difference representation: Write the finite difference representation of the heat equation : \\(u_{t} = Du_{xx}\\), using backward difference in time and central difference in space. \\[\\dfrac{u_n^{m} - u_n^{m-1}}{\\Delta t} = \\dfrac{u_{n-1}^{m} - 2u_n^{m} + u_{n+1}^{m}}{\\Delta x^2}\\] Consider the LHS. It is not possible to take the backward difference in time, hence we let \\(m = m + 1\\), thus our finite difference representation becomes: \\[\\dfrac{u_n^{m+1} - u_n^{m}}{\\Delta t} = \\dfrac{u_{n-1}^{m+1} - 2u_n^{m+1} + u_{n+1}^{m+1}}{\\Delta x^2}.\\] Let \\(\\mu = D\\dfrac{\\Delta t}{\\Delta x^2}\\) \\[ \\implies u_n^m = -\\mu u_{n-1}^{m+1} + (1+2\\mu)u_{n}^{m+1} -\\mu u_{n+1}^{m+1} \\] At \\(m=1 \\ \\forall n \\in \\left[0,N\\right]\\), th system to be solved is: \\[u_0^1 = \\alpha \\ \\ \\ \\text{(BC)}\\] \\[u_1^1 = -\\mu u_{0}^{2} + (1+2\\mu)u_{1}^{2} -\\mu u_{2}^{2}\\] \\[u_2^1 = -\\mu u_{1}^{2} + (1+2\\mu)u_{2}^{2} -\\mu u_{3}^{2}\\] \\[u_3^1 = -\\mu u_{2}^{2} + (1+2\\mu)u_{3}^{2} -\\mu u_{4}^{2}\\] \\[u_4^1 = -\\mu u_{3}^{2} + (1+2\\mu)u_{4}^{2} -\\mu u_{5}^{2}\\] \\[u_5^1 = \\beta \\ \\ \\ \\text{(BC)}\\] Solving the scheme: Calculate the following: \\(x\\) values \\(t\\) values Initial values ie. \\(u_n^0, \\ \\forall n \\in \\left[0, N\\right]\\) # x values xvals = np.linspace(0, 1, int(1/dx) + 1) numX = int(len(xvals)) # This is N, the number of x values at which we are evaluating the scheme. # t values tf = M*dt # This is the final time ie. when the algorithm terminates. tvals = np.linspace(0, tf, M + 1) # t values numT = int(len(tvals)) # This is M, the number of t values for which we are evaluating the scheme. #initial values u = f(xvals) # m = 0 #Boundary Conditions u[0] = alpha u[-1] = beta #constat: mu mu = D*(dt/dx**2) print(xvals) ## [0. 0.2 0.4 0.6 0.8 1. ] print(numX) ## 6 \\(x\\) values: \\(m\\) 0 1 2 3 4 \\(t\\) 0 0.01 0.02 0.03 0.04 Next we setup the system: \\[A\\underline{u}_n^{m+1} = \\underline{u}_n^{m}.\\] The dimension of the coefficient matrix \\(A\\), is numX \\(\\times\\) numX. In this example, the dimension of \\(A\\) is \\(6\\times 6\\). \\(A = \\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ -\\mu &amp; (1+2\\mu) &amp; -\\mu &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -\\mu &amp; (1+2\\mu) &amp; -\\mu &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -\\mu &amp; (1+2\\mu) &amp; -\\mu &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -\\mu &amp; (1+2\\mu) &amp; -\\mu \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}\\) Note: The first row, first element and the last row last element are equal to 1 to account for the boundary conditions. \\[ \\underline{u}_n^{m+1} = \\begin{bmatrix} {u}_0^{m+1}\\\\ {u}_1^{m+1}\\\\ {u}_2^{m+1}\\\\ {u}_3^{m+1}\\\\ {u}_4^{m+1} \\end{bmatrix}, \\qquad \\underline{u}_n^{m} = \\begin{bmatrix} {u}_0^{m}\\\\ {u}_1^{m}\\\\ {u}_2^{m}\\\\ {u}_3^{m}\\\\ {u}_4^{m} \\end{bmatrix} \\] #Coeffecient Matrix A = np.zeros((numX, numX)) coeff1 = -mu coeff2 = 1+(2*mu) coeff3 = -mu # OPTION 1 #fill coefficients on tridiagonal for i in range(1, numX-1): A[i,i-1] = coeff1 A[i,i] = coeff2 A[i,i+1] = coeff3 #first and last row A[0,0] = 1 A[-1,-1] = 1 # OPTION 2 # #Alternatively # A = sc.diags([coeff1[1::], coeff2, coeff3[:-1]], [-1,0,1], format=&#39;csc&#39;).toarray() # A[0,:] = 0 # A[-1,:] = 0 # #first and last row # A[0,0] = 1 # A[-1,-1] = 1 print(A) ## [[ 1. 0. 0. 0. 0. 0. ] ## [-0.25 1.5 -0.25 0. 0. 0. ] ## [ 0. -0.25 1.5 -0.25 0. 0. ] ## [ 0. 0. -0.25 1.5 -0.25 0. ] ## [ 0. 0. 0. -0.25 1.5 -0.25] ## [ 0. 0. 0. 0. 0. 1. ]] Solution Matrix \\(U\\): \\(U\\) stores the output after each time step \\(m, \\ \\forall m \\in \\left[0, M\\right].\\) The dimensions of \\(U\\): number of time steps + 1 \\(\\times\\) number of spatial steps + 1 ie. \\(M+1 \\times N +1.\\) That is numT \\(\\times\\) numX = \\(5 \\times 6\\). Note: rows \\(\\times\\) cols. \\[ U = \\begin{bmatrix}u_0^0 &amp; u_1^0 &amp; u_2^0 &amp; u_3^0 &amp; u_4^0 &amp; u_5^0\\\\ u_0^1 &amp; u_1^1 &amp; u_2^1 &amp; u_3^1 &amp; u_4^1 &amp; u_5^1\\\\ u_0^2 &amp; u_1^2 &amp; u_2^2 &amp; u_3^2 &amp; u_4^2 &amp; u_5^2\\\\ u_0^3 &amp; u_1^3 &amp; u_2^3 &amp; u_3^3 &amp; u_4^3 &amp; u_5^3\\\\ u_0^4 &amp; u_1^4 &amp; u_2^4 &amp; u_3^4 &amp; u_4^4 &amp; u_5^4\\\\ \\end{bmatrix} \\] #Initialise the solution matrix U = np.zeros((numT, numX)) #first row, where m = 0, contains the solutions calculated using the initial condition U[0,:] = u U ## array([[0. , 0.0001234, 0.3678794, 0.3678794, 0.0001234, 0. ], ## [0. , 0. , 0. , 0. , 0. , 0. ], ## [0. , 0. , 0. , 0. , 0. , 0. ], ## [0. , 0. , 0. , 0. , 0. , 0. ], ## [0. , 0. , 0. , 0. , 0. , 0. ]]) import numpy.linalg as LA # Next we calculate the solution when m = 1,2,...M for i in range(1,numT): u_updated = LA.inv(A)@u #solve for updated values in u^{m+1}, thus solve inv(A)u^{m} U[i,:] = u_updated #add updated values of u at m=i to the solution matrix U u = u_updated #update u vector to have updated values of u Solution: The final solution which we want is the solution matrix \\(U\\). We can plot this to see how the solution evolves over time. print(U) ## [[0. 0.0001234 0.3678794 0.3678794 0.0001234 0. ] ## [0. 0.0508271 0.304469 0.304469 0.0508271 0. ] ## [0. 0.0770489 0.258985 0.258985 0.0770489 0. ] ## [0. 0.0888592 0.2249598 0.2249598 0.0888592 0. ] ## [0. 0.0923112 0.1984301 0.1984301 0.0923112 0. ]] As an exercise manipulate the value of \\(dx\\) and see its effect on the solution produced. "],
["crank-nicolson-method.html", "4.4 Crank-Nicolson method", " 4.4 Crank-Nicolson method The Crank-Nicolson method essentially makes use of approximations to values halfway between time nodes by taking averages of the point values at the present time and the next. For example \\(v_t = v_{xx}\\) becomes \\[\\begin{equation} \\frac{u_n^{m+1} - u_n^m}{\\triangle{t}} = \\frac{u_{n+1}^{m+1} - 2 u_n^{m+1} + u_{n-1}^{m+1}}{2 \\triangle{x}^2} + \\frac{u_{n+1}^{m} - 2 u_n^{m} + u_{n-1}^{m}}{2 \\triangle{x}^2}. \\end{equation}\\] As such this scheme originates from combining an explicit and implicit scheme which allows us to find the following difference equation \\[\\begin{equation} -\\beta u_{n-1}^{m+1} + (2+2\\beta )u_n^{m+1} - \\beta u_{n+1}^{m+1} = \\beta u_{n-1}^{m} + (2-2\\beta )u_n^{m} + \\beta u_{n+1}^{m}. \\end{equation}\\] which equates three unknown values to three known values. This difference scheme can be set up in a similar fashion as was done for the implicit scheme (4.3). The system takes the form \\(B{\\bf u}^{m+1} = A{\\bf u}^m + {\\bf b}\\). Given some boundary values at the level \\(t_m\\) we can set up the system of equations \\[\\begin{equation*} \\left( \\begin{array}{cccccc} \\hat \\kappa &amp; \\hat \\kappa &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ -\\beta &amp; (2+2\\beta )&amp; -\\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; -\\beta &amp; (2+2\\beta ) &amp; -\\beta \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\hat \\kappa &amp; \\hat \\kappa \\end{array} \\right) \\qquad \\left( \\begin{array}{c} u_0^{m+1} \\\\ u_1^{m+1} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N}^{m+1} \\end{array} \\right) \\end{equation*}\\] \\[\\begin{equation*} = \\left( \\begin{array}{cccccc} \\hat \\kappa &amp; \\hat \\kappa &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\beta &amp; (2-2\\beta )&amp; \\beta &amp; \\ldots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp; \\ldots \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\beta &amp; (2-2\\beta ) &amp; \\beta \\\\ \\ldots &amp; \\ldots &amp; \\ldots &amp; \\ldots &amp; \\hat \\kappa &amp; \\hat \\kappa \\end{array} \\right) \\qquad \\left( \\begin{array}{c} u_0^{m} \\\\ u_1^{m} \\\\ \\vdots \\\\ \\vdots \\\\ \\vdots \\\\ u_{N}^{m} \\end{array} \\right) \\end{equation*}\\] \\[\\begin{equation*} + \\left( \\begin{array}{c} \\hat \\kappa \\\\ 0 \\\\ \\vdots \\\\ \\vdots \\\\ 0 \\\\ \\hat \\kappa \\end{array} \\right). \\tag{4.4} \\end{equation*}\\] Advantages of using this implicit scheme rather than an explicit scheme include: The truncation error is less than explicit schemes. The truncation error is \\(\\mathcal O \\left(\\dfrac{\\Delta t}{4}\\right)^2\\) + \\(\\mathcal O (\\Delta x)^2\\). The scheme is unconditionally convergent and stable. Thus, the restriction of \\(\\beta &lt; \\dfrac{1}{2}\\) is not to be satisfied. However, care is still to be taken that \\(\\beta\\) cannot be too lage. As per the derivation of the derivativein terms of finite differences, both \\(\\Delta t\\) and \\(\\Delta x\\) are quite small and \\(\\beta = \\dfrac{\\Delta t}{\\left(\\Delta x\\right)^2}\\), so obviously \\(\\beta\\) cannot be too large. Disadvantage of using this scheme: The equations are to be solved simultaneously at every timelevel which is time consuming. Note, the coeffecients matrix is tridiagonal, therefore an effecient solver to solve tridiagonal systems can be used to save computational time. FIGURE 4.3: Crank-Nicolson Scheme Stencil Example 4.8 Solve the heat equation \\[ V_t = V_{xx} \\ \\ 0\\leq x \\leq 1, \\ \\forall t \\geq 0 \\] Subject to: \\[ V(x,0) = \\sin \\dfrac{3\\pi x}{2}, \\qquad V(0,t) = 0\\ \\text{and}\\ V_x(1,t) = 0 \\] Use a the step sizes, \\(\\Delta x = 0.1\\), \\(\\Delta t = 0.0025\\) and perform 10 iterations. #given information delta_x = 0.1 delta_t = 0.0025 beta = 0.25 x_vals = np.linspace(0,1, int(1/delta_x) + 1) # x values in incremental steps of delta x from 0 to 1. numX = int(len(x_vals)) numT = 10 t_vals = np.arange(0,(numT+1)*delta_t, delta_t) Finite Difference Representation: \\[ -\\beta u_{n-1}^{m+1} + (2+2\\beta)u_n^{m+1} - \\beta u_{n+1}^{m+1} = \\beta u_{n-1}^{m} + (2-2\\beta)u_n^{m} + \\beta u_{n+1}^{m} \\] The equations can be written as follows once substituting \\(\\beta = 0.25\\) \\[n = 0: \\ \\ u_0^m = 0 \\ \\forall m \\ \\text{(Boundary Condition)}\\] \\[ n = 1: \\ \\ -0.25 u_0^1 + 2.5u_1^{1} - 0.25u_{2}^{1} = 0.25 u_0^0 + 1.5 u_1^{0} + 0.25 u_{2}^{0}\\] \\[n = 2: \\ \\ -0.25 u_{1}^{1} + 2.5u_2^{1} - 0.25u_{3}^{1} = 0.25 u_{1}^{0} + 1.5 u_2^{0} + 0.25 u_{3}^{0}\\] \\[n = 3: \\ \\ -0.25 u^{1}_{2} + 2.5u^1_{3} - 0.25u^{1}_{4} = 0.25 u^{0}_{2} + 1.5 u^0_{3} + 0.25 u^{0}_{4}\\] \\[n = 4: \\ \\ -0.25 u^{1}_{3} + 2.5u^1_{4} - 0.25u^{1}_{5} = 0.25 u^{0}_{3} + 1.5 u^0_{4} + 0.25 u^{0}_{5}\\] \\[n = 5: \\ \\ -0.25 u^{1}_{4} + 2.5u^1_{5} - 0.25u^{1}_{6} = 0.25 u^{0}_{4} + 1.5 u^0_{5} + 0.25 u^{0}_{6}\\] \\[ n = 6: \\ \\ -0.25 u^{1}_{5} + 2.5u^1_{6} - 0.25u^{1}_{7} = 0.25 u^{0}_{5} + 1.5 u^0_{6} + 0.25 u^{0}_{7}\\] \\[n = 7: \\ \\ -0.25 u^{1}_{6} + 2.5u^1_{7} - 0.25u^{1}_{8} = 0.25 u^{0}_{6} + 1.5 u^0_{7} + 0.25 u^{0}_{8}\\] \\[ n = 8: \\ \\ -0.25 u^{1}_{7} + 2.5u^1_{8} - 0.25u^{1}_{9} = 0.25 u^{0}_{7} + 1.5 u^0_{8} + 0.25 u^{0}_{9}\\] \\[n = 9: \\ \\ -0.25 u^{1}_{8} + 2.5u^1_{9} - 0.25u^{1}_{10} = 0.25 u^{0}_{8} + 1.5 u^0_{9} + 0.25 u^{0}_{10}\\] \\[n = N = 10: \\ \\ -0.25 u^{1}_{9} + 2.5u^1_{10} - 0.25u^{1}_{11} = 0.25 u^{0}_{9} + 1.5 u^0_{10} + 0.25 u^{0}_{11}\\] For the boundary condition at \\(n+1 = N\\), where \\(n=9\\), replacing the LHS with Backward Difference: \\[\\begin{eqnarray} \\dfrac{u_{N-1}^{m} - u_{N}^m}{\\Delta x} &amp;=&amp; 0 \\\\ \\implies u_{N-1}^{m} &amp;=&amp; u_N^m$ \\\\ \\implies u_{9}^{m} &amp;=&amp; u_{10}^m \\end{eqnarray}\\] Thus, the finite difference representation for when \\(n+1 = N = 10\\) becomes: \\[ -2\\beta u_{n-1}^{m+1} + (2+2\\beta)u_{n}^{m+1} = 2\\beta u_{n-1}^{m} + (2-2\\beta )u_{n}^{m} \\] \\[ \\implies -0.25u_8^{1} + 2.25u_{9}^{1} = 0.25 u_8^{0} + 1.75 u_{9}^{0} \\] \\[ A^{m+1} \\underline{u}_n^{m+1} = A^{m}\\underline{u}_n^{m} \\] \\[\\begin{equation}\\scriptsize \\begin{bmatrix} 2.5 &amp; -0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ -0.25 &amp; 2.5 &amp; -0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -0.25 &amp; 2.5 &amp; -0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -0.25 &amp; 2.5 &amp; -0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -0.25 &amp; 2.5 &amp; -0.25 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -0.25 &amp; 2.5 &amp; -0.25 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -0.25 &amp; 2.5 &amp; -0.25 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -0.25 &amp; 2.5 &amp; -0.25 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -0.25 &amp; 2.25 \\end{bmatrix} \\begin{bmatrix} {u}_1^{m+1}\\\\ {u}_2^{m+1}\\\\ {u}_3^{m+1}\\\\ {u}_4^{m+1}\\\\ {u}_5^{m+1}\\\\ {u}_6^{m+1}\\\\ {u}_7^{m+1}\\\\ {u}_8^{m+1}\\\\ {u}_9^{m+1} \\end{bmatrix} = \\\\ \\scriptsize \\begin{bmatrix} 1.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0.25 &amp; 1.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; -0.25 &amp; 1.5 &amp; 0.25&amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp;0.25 &amp; 1.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.25 &amp; 1.5 &amp; 0.25 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.25 &amp; 1.5 &amp; 0.25 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.25 &amp; 1.5 &amp; 0.25&amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.25 &amp; 1.5 &amp; 0.25 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.25 &amp; 1.75 \\\\ \\end{bmatrix} \\begin{bmatrix} {u}_1^{m}\\\\ {u}_2^{m}\\\\ {u}_3^{m}\\\\ {u}_4^{m}\\\\ {u}_5^{m}\\\\ {u}_6^{m}\\\\ {u}_7^{m}\\\\ {u}_8^{m}\\\\ {u}_9^{m}\\\\ \\end{bmatrix} \\end{equation}\\] Note: the LHS cosists of values of \\(u\\) at the \\(m+1\\) temporal level, whereas the RHS contains values of at the temporal level \\(m\\). Furthermore, the LHS contains unknown whereas all the values on the RHS are known for \\(n = 1, 2, ..., N\\) and \\(m = 1, 2,..., M\\). Also, \\(n = 0\\) and \\(m = 0\\) respectively define the boundary and initial conditions. Taking the boundary conditions into consideration, the followng coeffecient matricies \\(A\\) are set up for at \\(m\\) and \\(m+1\\): # For A_m delta1_LHS = 0.25 delta2_LHS = 1.5 delta3_LHS = 0.25 # For A_m1 delta1_RHS = -0.25 delta2_RHS = 2.5 delta3_RHS = -0.25 A_m = np.zeros((numX-2, numX-2)) A_m1 = np.zeros((numX-2, numX-2)) for n in range(1, numX-2-1): A_m[n, n-1] = delta1_LHS A_m[n, n] = delta2_LHS A_m[n, n+1] = delta3_LHS A_m1[n, n-1] = delta1_RHS A_m1[n, n] = delta2_RHS A_m1[n, n+1] = delta3_RHS A_m[0,0] = delta2_LHS A_m[0,1] = delta3_LHS A_m[-1,-2] = 0.25 A_m[-1,-1] = 1.75 A_m1[0,0] = delta2_RHS A_m1[0,1] = delta3_RHS A_m1[-1,-2] = -0.25 A_m1[-1,-1] = 2.25 A_m ## array([[1.5 , 0.25, 0. , 0. , 0. , 0. , 0. , 0. , 0. ], ## [0.25, 1.5 , 0.25, 0. , 0. , 0. , 0. , 0. , 0. ], ## [0. , 0.25, 1.5 , 0.25, 0. , 0. , 0. , 0. , 0. ], ## [0. , 0. , 0.25, 1.5 , 0.25, 0. , 0. , 0. , 0. ], ## [0. , 0. , 0. , 0.25, 1.5 , 0.25, 0. , 0. , 0. ], ## [0. , 0. , 0. , 0. , 0.25, 1.5 , 0.25, 0. , 0. ], ## [0. , 0. , 0. , 0. , 0. , 0.25, 1.5 , 0.25, 0. ], ## [0. , 0. , 0. , 0. , 0. , 0. , 0.25, 1.5 , 0.25], ## [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.25, 1.75]]) A_m1 ## array([[ 2.5 , -0.25, 0. , 0. , 0. , 0. , 0. , 0. , 0. ], ## [-0.25, 2.5 , -0.25, 0. , 0. , 0. , 0. , 0. , 0. ], ## [ 0. , -0.25, 2.5 , -0.25, 0. , 0. , 0. , 0. , 0. ], ## [ 0. , 0. , -0.25, 2.5 , -0.25, 0. , 0. , 0. , 0. ], ## [ 0. , 0. , 0. , -0.25, 2.5 , -0.25, 0. , 0. , 0. ], ## [ 0. , 0. , 0. , 0. , -0.25, 2.5 , -0.25, 0. , 0. ], ## [ 0. , 0. , 0. , 0. , 0. , -0.25, 2.5 , -0.25, 0. ], ## [ 0. , 0. , 0. , 0. , 0. , 0. , -0.25, 2.5 , -0.25], ## [ 0. , 0. , 0. , 0. , 0. , 0. , 0. , -0.25, 2.25]]) A = np.dot(LA.inv(A_m1), A_m) Initial condition # Initial Solution u = np.sin((3*x_vals*np.pi)/2) u[0] = 0 #BC1 u[-1] = u[-2] # BC2 Solution matrix \\(U\\) U = np.zeros((numT, numX)) U[0,:] = u Update solution matrix \\(U\\) for each temporal step, \\(m+1\\), \\(m+2\\)… u_ = u[1:-1] for m in range(1, numT): u_updated = A@u_ U[m, 1:-1] = u_updated U[m, -1] = u_updated[-1]# Boundary condition u_ = u_updated U[0,:] # Initial Condition: u vector at m = 0 for all n ## array([ 0. , 0.4539905, 0.809017 , 0.9876883, 0.9510565, ## 0.7071068, 0.309017 , -0.1564345, -0.5877853, -0.8910065, ## -0.8910065]) We can visualise the temperature change over time in 2D below: Similarly, we can see this behaviour in the surface plot below: 4.4.1 Tutorial Implement the Crank-Nicolson finite difference method on \\(u_t=u_{xx}\\) with the boundary conditions \\(u(0,t) = u(1,t) = 0\\) and prove that the method is unconditionally stable. Burden &amp; Faires, Exercise Set 12.2, No. 1, 2, 3 (a) and (b) Implement an explicit scheme on Example 4.8 and compare your results to the Crank-Nicolson scheme. What observations can you make. Consider the PDE: \\[V_t = x V_{xx} \\ \\ 0&lt; x &lt; 1, \\ \\forall t &gt; 0\\] Subject to boundary conditions: \\(V(0,t) = 0\\) and \\(V_x(1,t) = -0.5V \\ \\,\\forall t&gt;0\\) and initial condition: \\(V(x,0) = x(1-x)\\). Solve the equation using Crank-Nicholson Scheme, employing central difference for the boundary conditions. Take \\(\\Delta x = h = 0.1\\) and \\(\\beta = 0.25\\). "],
["alternating-direction-implicit-adi-schemes.html", "4.5 Alternating Direction Implicit (ADI) Schemes", " 4.5 Alternating Direction Implicit (ADI) Schemes 4.5.1 Motivation for ADI Just as the Crank-Nicolson (CN) method for PDEs with one space dimension, the Alternating Direction Implicit (ADI) method is used commonly for PDEs with two or three space dimensions. When solving the 2D Heat Equation numerically, we can still make use of the CN stencil. For example, applying the CN stencil to our 2D Heat Equation on the \\((x, y, n)\\) grid, we get: \\[\\begin{eqnarray} \\frac{u_{i, j}^{n+1}-u_{i, j}^{n}}{\\Delta t}&amp;=&amp;\\frac{D}{2 \\Delta x^{2}}\\left(u_{i, j+1}^{n}-2 u_{i, j}^{n}+u_{i, j-1}^{n}+u_{i, j+1}^{n+1}-2 u_{i, j}^{n+1}+u_{i, j-1}^{n+1}\\right)\\\\ &amp;+&amp;\\frac{D}{2 \\Delta y^{2}}\\left(u_{i+1, j}^{n}-2 u_{i, j}^{n}+u_{i-1, j}^{n}+u_{i+1, j}^{n+1}-2 u_{i, j}^{n+1}\\right) \\end{eqnarray}\\] In order to map our stencil into a linear system, we need to define a new index that combines indices \\(i\\) and \\(j\\): \\[ k = j + iJ, \\] where \\(J\\) represents the number of grid points in the \\(x (i)\\) direction. Effectively, the introduced index \\(k\\) flattens the two dimensional spatial component of the \\((x, y, t)\\) grid into a 1D vector. For example, when we are accessing the \\(k\\) grid points \\((i, j, n)\\) and \\((i, j + 1, n)\\), they become \\((k, n)\\) and \\((k + J, n)\\) respectively. Thus the stencil becomes: \\[\\begin{eqnarray} u_{k}^{n+1}-u_{k}^{n}&amp;=&amp;\\sigma_{x}\\left(u_{k+1}^{n}-2 u_{k}^{n}+u_{k-1}^{n}+u_{k+1}^{n+1}-2 u_{k}^{n+1}+u_{k}^{n+1}\\right) \\\\ &amp;+&amp;\\sigma_{y}\\left(u_{k+1}^{n}-2 u_{k}^{n}+u_{k-j}^{n}+u_{k+1}^{n+1}+u_{k-1}^{n+1}\\right), \\end{eqnarray}\\] where \\(\\sigma_x = \\frac{D\\Delta t}{2\\Delta x^2}\\) and \\(\\sigma_y = \\frac{D\\Delta t}{2\\Delta y^2}\\). Reordering, we obtain: \\[\\begin{eqnarray} &amp;&amp;-\\sigma_{y} u_{k-J}^{n+1}-\\sigma_{x} u_{k-1}^{n+1}+\\left(1+2 \\sigma_{x}+2 \\sigma_{y}\\right) u_{k}^{n+1}-\\sigma_{x} u_{k+1}^{n+1}-\\sigma_{y} u_{k+J}^{n+1} \\\\ &amp;=&amp;\\sigma_{y} u_{k-J}^{n}+\\sigma_{x} u_{k-1}^{n}+\\left(1-2 \\sigma_{x}-2 \\sigma_{y}\\right) u_{k}^{n}+\\sigma_{x} u_{k+1}^{n}+\\sigma_{y} u_{k+J}^{n}.\\tag{4.5} \\end{eqnarray}\\] We can see that if we wrote out Equation (4.5), we would get banded matrices on both the right and left-hand sides. Importantly, the numerical inversion of these 5 point banded matrices is computationally more expensive than the tridiagonal seen in the 1D case. Additionally, we now have to solve these block banded matrix equations, which are systems of much lareger size than what we have dealt with previously. The ADI stencil then, utilises a trick to allow us to invert tridiagonal matrices, as was the case in the 1D CN stencil instead of banded matrices. 4.5.2 Peaceman-Rachford scheme The implicit difference scheme (4.3) is unconditionally stable, as seen before. If we choose to solve the implicit difference scheme as a general matrix equation such as \\[\\begin{equation} A {\\bf u}^{m+1} = {\\bf u}^m + \\triangle{t} {\\bf G}^{m+1} \\tag{4.6} \\end{equation}\\] then we must solve a broadly banded matrix equation. In general we would not want to solve an equation such as (4.6) through the use of Gaussian Elimination or other well known iterative methods. A way around solving an equation such as this is to use an alternating direction implicit (ADI) scheme. Using such a scheme can be interpreted as either approximating the solution to equation (4.6) or as using equation (4.6) to help us develop a new and hopefully better scheme. ADI schemes have the advantages of an implicit schemes and require that we solve only tridiagonal matrices computationally. What is the point? We would like to have an unconditionally stable scheme that involves linear systems with a tridiagonal matrix! Let us now consider how to construct an ADI scheme by considering the following ordinary differential equation \\[\\begin{equation} \\frac{du}{dt} = (A_1+A_2) u \\qquad u(0) = u_0. \\end{equation}\\] Operators \\(A\\) in this instance will represent numerical derivatives. Using the trapezoide rule we can write: \\[\\begin{equation} u_{n+1} = u_n + \\frac{\\triangle{t}}{2}[\\left(A_1+A_2\\right)u_{n+1} +\\left(A_1+A_2\\right)u_{n} ] \\tag{4.7} \\end{equation}\\] which implies that \\[\\begin{equation} [I-\\frac{\\triangle{t}}{2}\\left(A_1+A_2\\right)]u_{n+1} = [I+\\frac{\\triangle{t}}{2}\\left(A_1+A_2\\right)]u_{n}. \\tag{4.8} \\end{equation}\\] From (4.8) we create two stages: 1st stage: Take a half-step implicitly in the direction \\(A_1\\) and explicitly in the direction \\(A_2\\) \\[\\begin{equation} u_{n+\\frac{1}{2}} = u_n + \\frac{\\triangle{t}}{2}[A_1 u_{n+\\frac{1}{2}} + A_2 u_n] \\tag{4.9} \\end{equation}\\] 2nd stage: Take another half-step implicitly in the direction \\(A_2\\) and explicitly in the direction \\(A_1\\) \\[\\begin{equation} u_{n+1} = u_{n+\\frac{1}{2}} + \\frac{\\triangle{t}}{2}[A_2 u_{n+1} + A_1 u_{n+\\frac{1}{2}}]. \\tag{4.10} \\end{equation}\\] Rearranging the two equations we obtain \\[[I-\\frac{\\triangle{t}}{2}A_1]u_{n+\\frac{1}{2}} = [I+\\frac{\\triangle{t}}{2}A_2]u_{n}\\] \\[\\begin{equation} [I-\\frac{\\triangle{t}}{2}A_2]u_{n+1} = [I+\\frac{\\triangle{t}}{2}A_1]u_{n+\\frac{1}{2}}. \\tag{4.11} \\end{equation}\\] This scheme has the stability of a Crank-Nicolson approximation and is correct to \\(\\mathcal{O}(\\triangle{t}^2)\\). Now set up an ADI scheme such as (4.11) for a partial differential equation. 4.5.3 The ADI Stencils The ADI method makes use of a time splitting approach, where the time step \\(\\Delta t\\) is split into two, and two different stencils are used for each half time step. Therefore, in the first step, we first compute an appoximation at \\(u_{ij}^{n + 1/2}\\), and then in the second step, an approximation for \\(u_{ij}^{n + 1}\\). Importantly, in both of these calculations, the operators used in the system are tridiagonal. The two ADI stencils which alternate in the \\(x\\) and \\(y\\) respectively (hence the name), are given below: \\[\\scriptsize \\frac{u_{i, j}^{n+1 / 2}-u_{i, j}^{n}}{\\Delta t / 2}=\\frac{D}{ \\Delta x^{2}}\\left(u_{i, j+1}^{n+1 / 2}-2 u_{i, j}^{n+1 / 2}+u_{i, j-1}^{n+1 / 2}\\right)+\\frac{D}{ \\Delta y^{2}}\\left(u_{i+1, j}^{n}-2 u_{i, j}^{n}+u_{i-1, j}^{n}\\right), \\tag{4.12} \\] \\[\\scriptsize \\frac{u_{i, j}^{n+1}-u_{i, j}^{n+1 / 2}}{\\Delta t / 2}=\\frac{D}{ \\Delta x^{2}}\\left(u_{i, j+1}^{n+1 / 2}-2 u_{i, j}^{n+1 / 2}+u_{i, j-1}^{n+1 / 2}\\right)+\\frac{D}{ \\Delta y^{2}}\\left(u_{i+1, j}^{n+1}-2 u_{i, j}^{n+1}+u_{i-1, j}^{n+1}\\right).\\tag{4.13} \\] Let \\(\\alpha_x = \\frac{D\\Delta t}{2\\Delta x^2}\\) and \\(\\alpha_y = \\frac{D\\Delta t}{2\\Delta y^2}\\), then we can reorder Equations (4.12) and (4.13) to obtain the following linear systems: \\[ -\\alpha_{x} u_{i, j-1}^{n+1 / 2}+\\left(1+2 \\alpha_{x}\\right) u_{i, j}^{n+1 / 2}-\\alpha_{x} u_{i, j+1}^{n+1 / 2}=\\alpha_{y} u_{i-1, j}^{n}+\\left(1-2 \\alpha_{y}\\right) u_{i, j}^{n}+\\alpha_{y} u_{i+1, j}^{n}, \\] \\[ -\\alpha_{y} u_{i+1, j}^{n+1}+\\left(1+2 \\alpha_{y}\\right) u_{i, j}^{n+1}-\\alpha_{y} u_{i-1, j}^{n+1}=\\alpha_{x} u_{i, j-1}^{n+1 / 2}+\\left(1-2 \\alpha_{x}\\right) u_{i, j}^{n+1 / 2}+\\alpha_{x} u_{i, j+1}^{n+1 / 2}. \\] We can now combine the above to write out the two-step linear systems in matrix form: \\[ Au_{x, j}^{n + 1/2} = Bu_{x, j}^{n}, \\] where: \\[ A=\\left[\\begin{array}{cccccccccc}{1+2\\alpha_{x}} &amp; {-\\alpha_{x}} &amp; {0} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {-\\alpha_{x}} &amp; {1+2 \\alpha_{x}} &amp; {-\\alpha_{x}} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {-\\alpha_{x}} &amp; {1+2 \\alpha_{x}} &amp; {-\\alpha_{x}} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {-\\alpha_{x}} &amp; {1+2 \\alpha_{x}} &amp; {-\\alpha_{x}} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {-\\alpha_{x}} &amp; {1+2\\alpha_{x}}\\end{array}\\right], \\] and \\[ B=\\left[\\begin{array}{cccccccccc}{1-2\\alpha_{y}} &amp; {\\alpha_{y}} &amp; {0} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {\\alpha_{y}} &amp; {1-2 \\alpha_{y}} &amp; {\\alpha_{y}} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {\\alpha_{y}} &amp; {1-2 \\alpha_{y}} &amp; {\\alpha_{y}} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {\\alpha_{y}} &amp; {1-2 \\alpha_{y}} &amp; {\\alpha_{y}} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {\\alpha_{y}} &amp; {1-2\\alpha_{y}}\\end{array}\\right]. \\] With step two: \\[ Cu_{y, i}^{n + 1} = Du_{y, i}^{n + 1/2}, \\] where: \\[ A=\\left[\\begin{array}{cccccccccc}{1+2\\alpha_{y}} &amp; {-\\alpha_{y}} &amp; {0} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {-\\alpha_{y}} &amp; {1+2 \\alpha_{y}} &amp; {-\\alpha_{y}} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {-\\alpha_{y}} &amp; {1+2 \\alpha_{y}} &amp; {-\\alpha_{y}} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {-\\alpha_{y}} &amp; {1+2 \\alpha_{y}} &amp; {-\\alpha_{y}} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {-\\alpha_{y}} &amp; {1+2\\alpha_{y}}\\end{array}\\right], \\] and: \\[ B=\\left[\\begin{array}{cccccccccc}{1-2\\alpha_{x}} &amp; {\\alpha_{x}} &amp; {0} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {\\alpha_{x}} &amp; {1-2 \\alpha_{x}} &amp; {\\alpha_{x}} &amp; {0} &amp; {0} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {\\alpha_{x}} &amp; {1-2 \\alpha_{x}} &amp; {\\alpha_{x}} &amp; {\\cdots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {\\ddots} &amp; {0} &amp; {0} &amp; {0} &amp; {0} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {\\alpha_{y}} &amp; {1-2 \\alpha_{x}} &amp; {\\alpha_{x}} \\\\ {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {0} &amp; {\\alpha_{x}} &amp; {1-2\\alpha_{x}}\\end{array}\\right]. \\] Example 4.9 Consider the PDE given by: \\[ v_t = D\\left( v_{xx} + v_{yy}\\right), \\] \\[ v(-1, y, t) = v(1, y, t) = v(x, -1, t) = v(x, 1, t) = 0, v(x, y, 0) = e^{-(x^2 + y^2)} \\] Implementing an ADI approach we obtain the following: dx = 0.1 dy = 0.1 dt = 0.01 D = 0.5 N = 10 U = heat2D_ADI(dx, dy, dt, D, N) print(U[:, :, -1]) ## [[0. 0. 0. 0. 0. 0. 0. ## 0. 0. 0. 0. 0. 0. 0. ## 0. 0. 0. 0. 0. 0. 0. ] ## [0. 0.0291588 0.0566352 0.0811422 0.1019955 0.1190652 0.1325576 ## 0.142774 0.1499541 0.1542301 0.155652 0.1542301 0.1499541 0.142774 ## 0.1325576 0.1190652 0.1019955 0.0811422 0.0566352 0.0291588 0. ] ## [0. 0.0566352 0.1100023 0.1576023 0.1981057 0.23126 0.2574662 ## 0.2773095 0.2912555 0.2995608 0.3023224 0.2995608 0.2912555 0.2773095 ## 0.2574662 0.23126 0.1981057 0.1576023 0.1100023 0.0566352 0. ] ## [0. 0.0811422 0.1576023 0.2257997 0.2838296 0.3313304 0.3688766 ## 0.3973064 0.4172871 0.4291862 0.4331429 0.4291862 0.4172871 0.3973064 ## 0.3688766 0.3313304 0.2838296 0.2257997 0.1576023 0.0811422 0. ] ## [0. 0.1019955 0.1981057 0.2838296 0.356773 0.4164813 0.4636768 ## 0.499413 0.5245287 0.5394858 0.5444593 0.5394858 0.5245287 0.499413 ## 0.4636768 0.4164813 0.356773 0.2838296 0.1981057 0.1019955 0. ] ## [0. 0.1190652 0.23126 0.3313304 0.4164813 0.4861823 0.5412763 ## 0.5829932 0.6123121 0.6297724 0.6355783 0.6297724 0.6123121 0.5829932 ## 0.5412763 0.4861823 0.4164813 0.3313304 0.23126 0.1190652 0. ] ## [0. 0.1325576 0.2574662 0.3688766 0.4636768 0.5412763 0.6026134 ## 0.6490577 0.681699 0.7011379 0.7076017 0.7011379 0.681699 0.6490577 ## 0.6026134 0.5412763 0.4636768 0.3688766 0.2574662 0.1325576 0. ] ## [0. 0.142774 0.2773095 0.3973064 0.499413 0.5829932 0.6490577 ## 0.6990814 0.7342385 0.7551755 0.7621375 0.7551755 0.7342385 0.6990814 ## 0.6490577 0.5829932 0.499413 0.3973064 0.2773095 0.142774 0. ] ## [0. 0.1499541 0.2912555 0.4172871 0.5245287 0.6123121 0.681699 ## 0.7342385 0.7711636 0.7931536 0.8004657 0.7931536 0.7711636 0.7342385 ## 0.681699 0.6123121 0.5245287 0.4172871 0.2912555 0.1499541 0. ] ## [0. 0.1542301 0.2995608 0.4291862 0.5394858 0.6297724 0.7011379 ## 0.7551755 0.7931536 0.8157706 0.8232912 0.8157706 0.7931536 0.7551755 ## 0.7011379 0.6297724 0.5394858 0.4291862 0.2995608 0.1542301 0. ] ## [0. 0.155652 0.3023224 0.4331429 0.5444593 0.6355783 0.7076017 ## 0.7621375 0.8004657 0.8232912 0.8308812 0.8232912 0.8004657 0.7621375 ## 0.7076017 0.6355783 0.5444593 0.4331429 0.3023224 0.155652 0. ] ## [0. 0.1542301 0.2995608 0.4291862 0.5394858 0.6297724 0.7011379 ## 0.7551755 0.7931536 0.8157706 0.8232912 0.8157706 0.7931536 0.7551755 ## 0.7011379 0.6297724 0.5394858 0.4291862 0.2995608 0.1542301 0. ] ## [0. 0.1499541 0.2912555 0.4172871 0.5245287 0.6123121 0.681699 ## 0.7342385 0.7711636 0.7931536 0.8004657 0.7931536 0.7711636 0.7342385 ## 0.681699 0.6123121 0.5245287 0.4172871 0.2912555 0.1499541 0. ] ## [0. 0.142774 0.2773095 0.3973064 0.499413 0.5829932 0.6490577 ## 0.6990814 0.7342385 0.7551755 0.7621375 0.7551755 0.7342385 0.6990814 ## 0.6490577 0.5829932 0.499413 0.3973064 0.2773095 0.142774 0. ] ## [0. 0.1325576 0.2574662 0.3688766 0.4636768 0.5412763 0.6026134 ## 0.6490577 0.681699 0.7011379 0.7076017 0.7011379 0.681699 0.6490577 ## 0.6026134 0.5412763 0.4636768 0.3688766 0.2574662 0.1325576 0. ] ## [0. 0.1190652 0.23126 0.3313304 0.4164813 0.4861823 0.5412763 ## 0.5829932 0.6123121 0.6297724 0.6355783 0.6297724 0.6123121 0.5829932 ## 0.5412763 0.4861823 0.4164813 0.3313304 0.23126 0.1190652 0. ] ## [0. 0.1019955 0.1981057 0.2838296 0.356773 0.4164813 0.4636768 ## 0.499413 0.5245287 0.5394858 0.5444593 0.5394858 0.5245287 0.499413 ## 0.4636768 0.4164813 0.356773 0.2838296 0.1981057 0.1019955 0. ] ## [0. 0.0811422 0.1576023 0.2257997 0.2838296 0.3313304 0.3688766 ## 0.3973064 0.4172871 0.4291862 0.4331429 0.4291862 0.4172871 0.3973064 ## 0.3688766 0.3313304 0.2838296 0.2257997 0.1576023 0.0811422 0. ] ## [0. 0.0566352 0.1100023 0.1576023 0.1981057 0.23126 0.2574662 ## 0.2773095 0.2912555 0.2995608 0.3023224 0.2995608 0.2912555 0.2773095 ## 0.2574662 0.23126 0.1981057 0.1576023 0.1100023 0.0566352 0. ] ## [0. 0.0291588 0.0566352 0.0811422 0.1019955 0.1190652 0.1325576 ## 0.142774 0.1499541 0.1542301 0.155652 0.1542301 0.1499541 0.142774 ## 0.1325576 0.1190652 0.1019955 0.0811422 0.0566352 0.0291588 0. ] ## [0. 0. 0. 0. 0. 0. 0. ## 0. 0. 0. 0. 0. 0. 0. ## 0. 0. 0. 0. 0. 0. 0. ]] 4.5.4 Stability One way to attempt to make the difference equations we have been working with nicer is to evaluate one of the spatial derivatives implicitly and the other explicitly - as per the ADI scheme already discussed. Computationally the scheme is simpler because we would be left with a tridiagonal matrix to solve. The question to ask is: would such a difference scheme be stable? We set up such a scheme by evaluating the derivative with respect to \\(x\\) implicitly, the derivative with respect to \\(y\\) explicitly and using a time step of \\(\\triangle{t}/2\\) to get the scheme \\[\\begin{equation} \\frac{u_{j,n}^{m+\\frac{1}{2}} - u_{j,n}^m}{\\triangle{t}/2} = \\frac{\\nu}{\\triangle{x}^2} \\delta^2_x u_{j,n}^{m+\\frac{1}{2}} + \\frac{\\nu}{\\triangle{y}^2}\\delta^2_y u_{j,n}^{m}. \\tag{4.14} \\end{equation}\\] We rewrite equation (4.14) as \\[\\begin{equation} \\left(1-\\frac{\\beta_x}{2}\\delta^2_x\\right) u_{j,n}^{m+\\frac{1}{2}} = \\left(1+\\frac{\\beta_y}{2}\\delta^2_y\\right) u_{j,n}^{m} \\tag{4.15} \\end{equation}\\] with \\(\\beta_x = \\triangle{t}/\\triangle{x}^2\\) and \\(\\beta_y = \\triangle{t}/\\triangle{y}^2\\). If we take the discrete Fourier transform of equation (4.15) we get \\[\\begin{equation} LHS: -\\beta\\dfrac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n\\xi} u^{m+\\frac{1}{2}}_{n-1,j} + (2+2\\beta)\\hat{u}^{m+\\frac{1}{2}}(\\xi) -\\beta\\dfrac{1}{\\sqrt{2 \\pi}} \\sum_{n=-\\infty}^{\\infty} e^{-i n\\xi} u^{m+\\frac{1}{2}}_{n+1,j} \\tag{4.16} \\end{equation}\\] \\[\\begin{equation} RHS:\\beta\\dfrac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n\\xi} u^{m}_{n,j+1} + (2\\beta-2)\\hat{u}^{m}(\\xi) +\\beta\\dfrac{1}{\\sqrt{2 \\pi}}\\sum_{n=-\\infty}^{\\infty} e^{-i n\\xi} u^{m}_{n,j+1} \\tag{4.17} \\end{equation}\\] By making the change of variables \\(g = n\\pm 1\\) and \\(f = j\\pm 1\\) we get \\[\\begin{align} \\dfrac{1}{\\sqrt{2 \\pi}} \\sum_{j=-\\infty}^{\\infty} e^{-i j\\eta} u^{m}_{j\\pm-1} &amp;= \\dfrac{1}{\\sqrt{2 \\pi}} \\sum_{f=-\\infty}^{\\infty} e^{-i (f\\mp 1)\\eta} u^{m}_{f}\\\\ &amp;= e^{\\pm i\\xi} \\sum_{f=-\\infty}^{\\infty} e^{-i f\\eta} u^{m}_{f} \\\\ &amp;= e^{\\pm i\\eta} \\hat{u}^{m}(\\eta). \\tag{4.18} \\end{align}\\] Equation (4.18) the becomes \\[\\begin{equation} \\left(1 +2\\beta_x \\sin^2\\frac{\\xi}{2}\\right)\\hat{u}^{m+\\frac{1}{2}} = \\left(1 -2\\beta_y \\sin^2\\frac{\\eta}{2}\\right)\\hat{u}^{m}. \\tag{4.19} \\end{equation}\\] In this way we can see that the symbol of the difference operator \\[\\begin{equation} \\rho(\\xi,\\eta) = \\frac{1 -2\\beta_y \\sin^2\\frac{\\eta}{2}}{1 +2\\beta_x \\sin^2\\frac{\\xi}{2}} \\tag{4.20} \\end{equation}\\] is \\(\\leq1\\). Using the fact that the minimum value of \\(\\rho\\) occurs at \\((\\xi,\\eta) = (0,\\pm \\pi)\\) we see that the scheme (4.15) is conditionally stable with a stability limit of \\(\\beta_y\\leq1\\). If we had used a full time-step \\(\\triangle{t}\\) then this condition would be \\(\\beta_y\\leq1/2\\). By examining \\(\\rho\\) given in (4.20) we see that if we take another half step (this is why we used a half a time-step \\(\\triangle{t}/2\\) in equation (4.14) of the form \\[\\begin{equation} \\frac{u_{j,n}^{m+1}-u_{j,n}^{m+\\frac{1}{2}}}{\\triangle{t}/2} = \\frac{\\nu}{\\triangle{x}^2} \\delta^2_x u_{j,n}^{m+\\frac{1}{2}} + \\frac{\\nu}{\\triangle{y}^2}\\delta^2_y u_{j,n}^{m+1} \\tag{4.21} \\end{equation}\\] (this time explicit in \\(x\\) and implicit in \\(y\\)) or \\[\\begin{equation} \\left(1-\\frac{\\beta_y}{2}\\delta^2_y\\right) u_{j,n}^{m+1} = \\left(1+\\frac{\\beta_x}{2}\\delta^2_x\\right) u_{j,n}^{m+\\frac{1}{2}} \\tag{4.22} \\end{equation}\\] and take the discrete Fourier transform of the result we get \\[\\left(1 +2\\beta_y \\sin^2\\frac{\\eta}{2}\\right)\\hat{u}^{m+1} = \\left(1 -2\\beta_x \\sin^2\\frac{\\xi}{2}\\right)\\hat{u}^{m+\\frac{1}{2}}\\] \\[\\begin{equation} = \\left(1 -2\\beta_x \\sin^2\\frac{\\xi}{2}\\right)\\frac{1 -2\\beta_y \\sin^2\\frac{\\eta}{2}}{1 +2\\beta_x \\sin^2\\frac{\\xi}{2}} \\hat{u}^m. \\tag{4.23} \\end{equation}\\] Thus the symbol of the two step difference scheme (4.15) and (4.22) is \\[\\begin{equation} \\rho(\\xi,\\eta) = \\frac{\\left(1 -2\\beta_x \\sin^2\\frac{\\xi}{2}\\right)\\left(1 -2\\beta_y \\sin^2\\frac{\\eta}{2}\\right)}{\\left(1 +2\\beta_x \\sin^2\\frac{\\xi}{2}\\right)\\left(1 +2\\beta_y \\sin^2\\frac{\\eta}{2}\\right)}. \\tag{4.24} \\end{equation}\\] We can easily see that \\(|\\rho|\\leq1\\). Thus the two step scheme (4.15) and (4.22), called Peaceman-Rachford scheme, is unconditionally stable. 4.5.5 Tutorial Another means of considering the stability of the Peaceman-Rachford scheme is through the use of the von Neumann stability analysis. Implement this means of ascertaining the stability of the scheme and prove that the Peaceman-Rachford scheme is unconditionally stable. 4.5.6 Consistency We now consider the accuracy (consistency) of scheme (4.15) and (4.22). If we operate on both sides of equation (4.15) by \\(\\left(1+\\frac{\\beta_x}{2} \\delta^2_x\\right)\\) we get \\[\\begin{equation} \\left(1+\\frac{\\beta_x}{2}\\delta^2_x\\right)\\left(1-\\frac{\\beta_x}{2}\\delta^2_x\\right)u_{j,n}^{m+\\frac{1}{2}} = \\left(1+\\frac{\\beta_x}{2}\\delta^2_x\\right)\\left(1+\\frac{\\beta_y}{2}\\delta^2_y\\right)u_{j,n}^{m}. \\tag{4.25} \\end{equation}\\] Since the product of the two operators on the left hand side commute, equation (4.25) can be written as \\[\\begin{equation} \\left(1-\\frac{\\beta_x}{2}\\delta^2_x\\right)\\left(1+\\frac{\\beta_x}{2}\\delta^2_x\\right)u_{j,n}^{m+\\frac{1}{2}} = \\left(1+\\frac{\\beta_x}{2}\\delta^2_x\\right)\\left(1+\\frac{\\beta_y}{2}\\delta^2_y\\right)u_{j,n}^{m}. \\tag{4.26} \\end{equation}\\] Equation (4.22) can be used to eliminate the \\(u_{j,n}^{m+\\frac{1}{2}}\\) term so that we can write the Peaceman-Rachford scheme as \\[\\begin{equation} \\left(1-\\frac{\\beta_x}{2}\\delta^2_x\\right)\\left(1-\\frac{\\beta_y}{2}\\delta^2_y\\right)u_{j,n}^{m+1} = \\left(1+\\frac{\\beta_x}{2}\\delta^2_x\\right)\\left(1+\\frac{\\beta_y}{2}\\delta^2_y\\right)u_{j,n}^{m}. \\tag{4.27} \\end{equation}\\] Expanding the terms in (4.27) we see that form (4.27) of the Peaceman-Rachford scheme is equivalent to \\[\\frac{u_{j,n}^{m+1} - u_{j,n}^m}{\\triangle{t}} = \\frac{\\nu}{2} \\frac{\\delta^2_x}{\\triangle{x}^2} \\left(u_{j,n}^m + u_{j,n}^{m+1}\\right)\\] \\[\\begin{equation} +\\frac{\\nu}{2} \\frac{\\delta^2_y}{\\triangle{y}^2} \\left(u_{j,n}^m + u_{j,n}^{m+1}\\right)-\\frac{\\nu^2 \\triangle{t}}{4} \\frac{\\delta^2_x}{\\triangle{x}^2} \\frac{\\delta^2_y}{\\triangle{y}^2}\\left(u_{j,n}^{m+1} - u_{j,n}^{m}\\right). \\tag{4.28} \\end{equation}\\] If we expand the last term in a Taylor series expansion form we find that \\[\\frac{\\delta^2_x}{\\triangle{x}^2} \\frac{\\delta^2_y}{\\triangle{y}^2}\\left(u_{j,n}^{m+1} - u_{j,n}^{m}\\right) \\] \\[\\begin{equation} = \\triangle{t}\\left(\\frac{\\partial ^5 u}{\\partial t \\partial^2 x \\partial^2 y}\\right)^m_{j,n} + \\mathcal{O}(\\triangle{t}\\triangle{x}^2) + \\mathcal{O}(\\triangle{t}\\triangle{y}^2) +\\mathcal{O}(\\triangle{t}^2). \\tag{4.29} \\end{equation}\\] Thus since the term expanded above is multiplied by \\(\\triangle{t}\\) in equation (4.28), the two step difference scheme (4.15) and (4.22) is second order accurate in \\(\\triangle{t}\\), \\(\\triangle{x}\\) and \\(\\triangle{y}\\). Of course, since we now have a consistent, stable scheme, we have a convergent scheme (as per the Lax Theorem). "],
["douglas-rachford-scheme.html", "4.6 Douglas-Rachford scheme", " 4.6 Douglas-Rachford scheme The Peaceman-Rachford scheme considered before is the approximate factorisation of the Crank-Nicolson scheme. We now develop the Douglas-Rachford scheme by actually approximately factoring the implicit scheme (BTCS - backward in time, centred in space). If we write the implicit scheme \\[\\begin{equation} u_{j,n}^{m+1} - \\beta_x \\delta_x^2 u_{j,n}^{m+1} - \\beta_y \\delta_y^2 u_{j,n}^{m+1} = u_{j,n}^m + \\triangle{t} F_{j,n}^{m+1} \\tag{4.30} \\end{equation}\\] without the nonhomogeneous term we obtain \\[\\begin{equation} (1-\\beta_x \\delta_x^2 - \\beta_y \\delta^2_y ) u_{j,n}^{m+1} = u_{j,n}^m. \\tag{4.31} \\end{equation}\\] We can now see that the left hand side naturally factors into \\[\\begin{equation} (1-\\beta_x \\delta_x^2)(1-\\beta_y \\delta_y^2) u_{j,n}^{m+1}. \\tag{4.32} \\end{equation}\\] Thus to factor equation () we must add a term of the form \\[\\begin{equation} \\beta_x \\beta_y \\delta_x^2 \\delta_y^2 u_{j,n}^{m+1}. \\tag{4.33} \\end{equation}\\] As would be done with the approximate factorization of the Crank-Nicolson scheme to get the Peaceman-Rachford scheme, we counter add the term (4.33) by adding the term \\[\\begin{equation} \\beta_x \\beta_y \\delta_x^2 \\delta_y^2 u_{j,n}^{m} \\tag{4.34} \\end{equation}\\] to the right hand side. Since the term which is being added is a higher order term the resulting difference equation has the same order accuracy as the implicit scheme - first order in \\(\\triangle{t}\\) and second order in \\(\\triangle{x}\\) and \\(\\triangle{y}\\). Thus, if we add (4.33) to the left hand side, (4.34) to the right hand side and factor, we obtain the Douglas-Rachford scheme as follows \\[\\begin{equation} (1-\\beta_x \\delta_x^2)(1-\\beta_y \\delta_y^2) u_{j,n}^{m+1} = (1+\\beta_x \\beta_y \\delta_x^2 \\delta_y^2) u_{j,n}^{m}. \\tag{4.35} \\end{equation}\\] The form of the Douglas-Rachford scheme used for computation is \\[\\begin{equation} (1-\\beta_x \\delta_x^2) u_{j,n}^{\\ast} = (1+\\beta_y \\delta_y^2) u_{j,n}^m \\tag{4.36} \\end{equation}\\] \\[\\begin{equation} (1-\\beta_y \\delta_y^2) u_{j,n}^{m+1} = u_{j,n}^{\\ast} - \\beta_y \\delta_y^2 u_{j,n}^m \\tag{4.37} \\end{equation}\\] which is equivalent to (4.35). For this scheme (4.36)-(4.37) we need to select the boundary conditions for \\(u^{\\ast}\\) carefully. Solving for \\(u^{\\ast}\\) from equation (4.37) gives \\[\\begin{equation} u_{j,n}^{\\ast} = (1-\\beta_y \\delta_y^2) u_{j,n}^{m+1} + \\beta_y \\delta_y^2 u_{j,n}^m \\tag{4.38} \\end{equation}\\] which means that the boundary condition for \\(u^{\\ast}\\) at \\(j=0\\) and \\(j=N_y\\) can be given in terms of the Dirichlet boundary conditions of \\(u\\) (for instance) by \\[\\begin{equation} u_{0,n}^{\\ast} = (1-\\beta_y \\delta_y^2) u_{0,n}^{m+1} + \\beta_y \\delta_y^2 u_{0,n}^m \\end{equation}\\] and \\[\\begin{equation} u_{N_y,n}^{\\ast} = (1-\\beta_y \\delta_y^2) u_{N_y,n}^{m+1} + \\beta_y \\delta_y^2 u_{N_y,n}^m. \\end{equation}\\] When the boundary conditions do not depend on time then the boundary conditions for \\(u^{\\ast}\\) will reduce to those for \\(u\\). 4.6.1 Tutorial Show, using the same methodology as that used in the section above, that the Peaceman-Rachford scheme considered before is the approximate factorisation of the Crank-Nicolson scheme. "],
["finite-difference-schemes-for-hyperbolic-pdes.html", "Chapter 5 Finite Difference Schemes for Hyperbolic PDEs", " Chapter 5 Finite Difference Schemes for Hyperbolic PDEs The wave equation is an example of a hyperbolic partial differential equation of the form \\[\\begin{equation} v_{tt} - \\alpha ^2 v_{xx} = 0, \\quad 0&lt;x&lt;l, \\quad t&gt;0 \\tag{5.1} \\end{equation}\\] subject to the conditions, \\[\\begin{equation} v(0,t) = v(l,t) = 0, \\quad t&gt;0, \\end{equation}\\] \\[\\begin{equation} v(x,0) = f(x), \\quad v_{t} |_{(x,0)} = g(x), \\quad 0\\leq x\\leq l \\end{equation}\\] where \\(\\alpha\\) is a constant. There is a vast range of physical processes which lead to wave motion. That is, signals which propagate through a medium in space and time with little to no permanent movement of the medium itself. Whilst the shape of signals may change as they traverse through a medium, this generally does not lead to a significant change that the signal is no longer recognisable at a later point in space and time. An example of the wave equation given in Equation (5.1) is the simulation of 1D waves on the string of a guitar. Here we are assuming that the deformed state of the string lays on the domain (0, \\(l\\)) on the \\(x\\)-axis, with \\(v(x, t)\\) the displacement of the string at time \\(t\\) in the direction “\\(y\\)”, i.e. height. "],
["lax-wendroff-scheme.html", "5.1 Lax-Wendroff Scheme", " 5.1 Lax-Wendroff Scheme Given that we are studying hyperbolic equations we consider the following model equation \\[\\begin{equation} v_t + a v_x = 0 \\tag{5.2} \\end{equation}\\] in an attempt to construct suitable difference schemes. One approach to attaining higher order stable schemes is to alter a centred scheme such as \\(u_n^{m+1} = u_n^m - \\frac{\\beta}{2} \\delta_0 u_n^m\\), where \\(\\beta = a \\triangle{t}/\\triangle{x}\\), to stabilize the scheme. We note that since \\(v_t = -a v_x\\) \\[v_{tt} = (-av_x)_t = -a v_{xt} = -a v_{tx} = -a(v_t)_x = -a(-av_x)_x = a^2 v_{xx}.\\] Then since, \\[(v)_n^{m+1} = v_n^m + (v_t)_n^m \\triangle{t} + (v_{tt})_n^m \\frac{\\triangle{t}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[\\hspace{2.4cm} = v_n^m + (-a v_x)_n^m \\triangle{t} + (a^2 v_{xx})_n^m \\frac{\\triangle{t}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[\\hspace{1.4cm} =v_n^m - a\\left(\\frac{v_{n+1}^m - v_{n-1}^m}{2 \\triangle{x}} + \\mathcal{O}(\\triangle{x}^2)\\right) \\triangle{t}\\] \\[\\hspace{3.4cm} + a^2\\left(\\frac{v_{n+1}^m - 2v_n^m + v_{n-1}^m}{\\triangle{x}^2} + \\mathcal{O}(\\triangle{x}^2)\\right) \\frac{\\triangle{t}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[\\begin{equation} \\hspace{5cm}= v_n^m - \\frac{a\\triangle{t}}{2\\triangle{x}}\\delta_0 v_n^m + \\frac{a^2 \\triangle{t}^2}{2 \\triangle{x}^2} \\delta^2v_n^m + \\mathcal{O}(\\triangle{t} \\triangle{x}^2) + \\mathcal{O}(\\triangle{t}^3) \\tag{5.3} \\end{equation}\\] we approximate the partial differential equation \\(v_t +a v_x=0\\) by the difference scheme \\[\\begin{equation} u_n^{m+1} = u_n^m - \\frac{\\beta}{2}\\delta_0u_n^m + \\frac{\\beta^2}{2}\\delta^2u_n^m \\tag{5.4} \\end{equation}\\] where \\(\\beta=a\\triangle{t}/\\triangle{x}\\). This scheme is called the Lax-Wendroff scheme. It is \\(\\mathcal{O}(\\triangle{t}^2)+\\mathcal{O}(\\triangle{x}^2)\\). Let us consider another example, namely the system of conservation laws \\[\\begin{equation} \\frac{\\partial {\\bf u}}{\\partial t} + \\frac{\\partial {\\bf f(u)}}{\\partial x} = 0 \\end{equation}\\] where \\({\\bf f(u)}\\) is the physical flux. Fundamentally, the basic idea is: expand \\({\\bf u}(x,t)\\) in a Taylor series to second order in time for fixed \\(x\\), use the partial differential equation to replace the time derivatives with spatial derivatives, and use central differences to approximate the resulting spatial derivative to second order. The resulting finite difference equation is then by construction second order accurate. Let \\({\\bf A} = d {\\bf f}/d{\\bf u}\\) be the Jacobi matrix for the flux function then we can expand \\({\\bf u}(x,t+\\triangle{t})\\) as follows: \\[{\\bf u}(x,t+\\triangle{t}) = {\\bf u}(x,t) + \\frac{\\partial {\\bf u}}{\\partial t}(x,t) {\\triangle{t}} + \\frac{\\partial^2 {\\bf u}}{\\partial t^2}(x,t) \\frac{{\\triangle{t}}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[{\\bf u}(x,t+\\triangle{t}) = {\\bf u}(x,t) - \\frac{\\partial {\\bf f(u)}}{\\partial x}(x,t) {\\triangle{t}} - \\frac{\\partial}{\\partial x}\\left( \\frac{\\partial {\\bf f(u)}}{\\partial t}\\right)(x,t) \\frac{{\\triangle{t}}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[{\\bf u}(x,t+\\triangle{t}) = {\\bf u}(x,t) - \\frac{\\partial {\\bf f(u)}}{\\partial x}(x,t) {\\triangle{t}} - \\frac{\\partial}{\\partial x} \\left(A\\frac{\\partial {\\bf u}}{\\partial t}\\right)(x,t) \\frac{{\\triangle{t}}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[{\\bf u}(x,t+\\triangle{t}) = {\\bf u}(x,t) - \\frac{\\partial {\\bf f(u)}}{\\partial x}(x,t) {\\triangle{t}} - \\frac{\\partial}{\\partial x} \\left(A\\frac{\\partial {\\bf f(u)}}{\\partial x}\\right)(x,t) \\frac{{\\triangle{t}}^2}{2} + \\mathcal{O}(\\triangle{t}^3)\\] \\[{\\bf u}(x,t+\\triangle{t}) = {\\bf u}(x,t) - \\frac{1}{2}\\frac{\\triangle{t}}{\\triangle{x}}\\left({\\bf f(u)}(x+\\triangle{x},t) - {\\bf f(u)}(x-\\triangle{x},t)\\right)+ \\] \\[ \\frac{1}{2} \\left(\\frac{\\triangle{t}}{\\triangle{x}}\\right)^2 \\left[{\\bf A(u)}\\left(x+\\frac{\\triangle{x}}{2},t\\right)\\left({\\bf f(u)}(x+\\triangle{x},t)-{\\bf f(u)}(x,t)\\right)\\right]-\\] \\[\\frac{1}{2} \\left(\\frac{\\triangle{t}}{\\triangle{x}}\\right)^2\\left[{\\bf A(u)}\\left(x-\\frac{\\triangle{x}}{2},t\\right)\\left({\\bf f(u)}(x,t)-{\\bf f(u)}(x-\\triangle(x),t)\\right)\\right]\\] \\[+ \\mathcal{O}(\\triangle{t}^3).\\] If we let \\(x=n \\triangle{x}\\) and \\(t=m\\triangle{t}\\) we obtain the difference method \\[{\\bf u}_n^{m+1} = {\\bf u}_n^m - \\frac{\\triangle{t}}{2 \\triangle{x}}\\left( {\\bf f}_{n+1}^m - {\\bf f}_{n-1}^m\\right) + \\] \\[ \\frac{1}{2}\\left(\\frac{\\triangle{t}}{\\triangle{x}}\\right)^2 \\left[{\\bf A}_{n+1/2}^m\\left({\\bf f}_{n+1}^m - {\\bf f}_n^m\\right) - {\\bf A}_{n-1/2}^m\\left({\\bf f}_n^m - {\\bf f}_{n-1}^m\\right)\\right]\\] \\[\\begin{equation} {\\bf f}_{n+1/2}^{m+1/2} = \\frac{{\\bf f}_{n+1}^m+{\\bf f}_n^m}{2} - \\frac{\\triangle{t}}{2\\triangle{x}}{\\bf A}_{n+1/2}^m\\left({\\bf f}_{n+1}^m - {\\bf f}_n^m\\right) \\end{equation}\\] where we let \\({\\bf A}_{n+1/2}^m = {\\bf A}\\left(\\left(u_{n+1}^m + u_n^m\\right)/2\\right)\\). In practise the Lax-Wendroff method is implemented as a two step method that is identical with the above formula for linear fluxes (i.e. constant {}), and is also second order in general \\[{\\bf u}_{n+1/2}^{m+1/2} = \\frac{{\\bf u}_{n+1}^m + {\\bf u}_n^m}{2} - \\frac{\\triangle{t}}{2\\triangle{x}} \\left({\\bf f}_{n+1}^m - {\\bf f}_n^m\\right)\\] \\[{\\bf f}_{n+1/2}^{m+1/2} = {\\bf f(u}_{n+1/2}^{m+1/2}{\\bf )}\\] \\[{\\bf u}_n^{m+1} = {\\bf u}_n^m - \\frac{\\triangle{t}}{\\triangle{x}}\\left({\\bf f}_{n+1/2}^{m+1/2} - {\\bf f}_{n-1/2}^{m+1/2}\\right).\\] It can be shown that for constant \\({\\bf A}\\) the Lax-Wendroff method is stable provided \\(|\\lambda|\\triangle{t}/\\triangle{x}&lt;1\\) for all eigenvalues of \\({\\bf A}\\). Remark: The implicit version of the Lax-Wendroff scheme is a second order accurate implicit scheme (second order in both time and space). The implicit version of the Lax-Wendroff scheme is hence given by \\[\\left(-\\frac{\\beta^2}{2} - \\frac{\\beta}{2}\\right)u_{n-1}^{m+1} + \\left(1+\\beta^2\\right)u_n^{m+1} + \\left(-\\frac{\\beta^2}{2} + \\frac{\\beta}{2}\\right)u_{n+1}^{m+1} = u_n^m.\\] Example 5.1 Consider the following one dimensional wave equation: \\[ \\dfrac{\\partial^2 u}{\\partial t^2} = \\nu^2\\dfrac{\\partial^2u}{\\partial x^2}. \\] Set up the system of coupled first-order conservative partial differential equations by structuring new dependent variables \\(r\\) and \\(s\\), which can be used to solve the equation above via the Lax-Wendroff difference scheme. Provide the system in vector notation where the system is represented as: \\[ \\dfrac{\\partial {\\bf U}}{\\partial t} + \\nabla {\\bf F}(\\bf U) = 0 \\] Solution: Let: \\[ r = \\nu\\frac{\\partial u}{\\partial x}, \\quad s = \\frac{\\partial u}{\\partial t}, \\] then: \\[ \\dfrac{\\partial r}{\\partial t} = \\nu\\dfrac{\\partial s}{\\partial x}, \\quad \\dfrac{\\partial s}{\\partial t} = \\nu\\dfrac{\\partial r}{\\partial x}. \\] \\[ \\dfrac{\\partial {\\bf U}}{\\partial t} + \\nabla {\\bf F}(\\bf U) = 0, \\] then: \\[ {\\bf U} = \\begin{bmatrix} r \\\\ s\\end{bmatrix}, \\quad {\\bf F}({\\bf U})\\begin{bmatrix}-\\nu &amp; 0\\\\ 0 &amp; -\\nu\\end{bmatrix}\\begin{bmatrix} r \\\\ s\\end{bmatrix} = -\\nu \\begin{bmatrix} r \\\\ s\\end{bmatrix}, \\] where: \\[ \\nabla = \\begin{bmatrix} 0 &amp; \\partial_x\\\\ \\partial_x &amp; 0\\end{bmatrix}. \\] 5.1.1 Tutorial Burden &amp; Faires, Exercise Set 12.2, No. 1, 2, 5 "],
["finite-difference-schemes-for-elliptic-pdes.html", "Chapter 6 Finite Difference Schemes for Elliptic PDEs", " Chapter 6 Finite Difference Schemes for Elliptic PDEs To add in 2020…. "],
["references.html", "References", " References "]
]
